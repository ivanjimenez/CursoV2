# Tema 14. BREVE INTRODUCCI√ìN AL TESTING

* [Tema 14. BREVE INTRODUCCI√ìN AL TESTING](Tema14.md#tema-14-breve-introducci√≥n-al-testing)
  * [14. Contenidos](Tema14.md#14-contenidos)
    * [14.1 Entorno de pruebas con Pytest en FastAPI](Tema14.md#141-entorno-de-pruebas-con-pytest-en-fastapi)
    * [14.2 Pruebas unitarias para servicios de dominio](Tema14.md#142-pruebas-unitarias-para-servicios-de-dominio)
    * [14.3 Simulaci√≥n de dependencias con `unittest.mock`](Tema14.md#143-simulaci√≥n-de-dependencias-con-unittestmock)
    * [14.4 `TestClient` para REST y WebSocket](Tema14.md#144-testclient-para-rest-y-websocket)
    * [14.5 Pruebas de integraci√≥n con DB temporal](Tema14.md#145-pruebas-de-integraci√≥n-con-db-temporal)
    * [14.6 Pruebas E2E entre microservicios](Tema14.md#146-pruebas-e2e-entre-microservicios)
    * [14.7 Validaci√≥n de eventos y colas en tests async](Tema14.md#147-validaci√≥n-de-eventos-y-colas-en-tests-async)
    * [14.8 Cobertura y calidad con `coverage.py`](Tema14.md#148-cobertura-y-calidad-con-coveragepy)
    * [14.9 Estructura de carpetas y fixtures](Tema14.md#149-estructura-de-carpetas-y-fixtures)
    * [14.10 Automatizaci√≥n en pipelines CI/CD](Tema14.md#1410-automatizaci√≥n-en-pipelines-cicd)
  * [Bibliograf√≠a](Tema14.md#bibliograf√≠a)

## 14. Contenidos

### 14.1 Entorno de pruebas con Pytest en FastAPI

Este es un tema absolutamente crucial. No importa cu√°n brillante sea el dise√±o de tu API, cu√°n eficientes sean tus `WebSockets` o cu√°n elegante sea tu l√≥gica de dominio; sin un conjunto robusto de pruebas, est√°s navegando en aguas turbulentas sin un mapa ni un salvavidas. El `testing` no es una ocurrencia tard√≠a, es una parte integral del desarrollo profesional de software.

Iniciaremos con el **14.1**, sentando las bases de nuestro entorno de pruebas con dos herramientas fundamentales en el ecosistema Python y FastAPI: `Pytest` y el `TestClient` de FastAPI.

***

Construir software es un arte, pero asegurar su calidad y fiabilidad es una ciencia. Las pruebas son el proceso mediante el cual validamos que nuestra aplicaci√≥n se comporta como esperamos, hoy y despu√©s de cada cambio futuro. En este tema, exploraremos diferentes niveles de pruebas:

* **`Unit Tests` (Pruebas Unitarias):** Verifican peque√±as piezas de c√≥digo aisladas (ej. una funci√≥n, un m√©todo de una clase).
* **`Integration Tests` (Pruebas de Integraci√≥n):** Verifican la interacci√≥n entre varios componentes (ej. tu API con la base de datos).
* **`End-to-End (E2E) Tests` (Pruebas de Extremo a Extremo):** Verifican flujos completos de la aplicaci√≥n desde la perspectiva del usuario.

Para orquestar todo esto, necesitamos un `test runner` y herramientas que faciliten la interacci√≥n con nuestra aplicaci√≥n FastAPI.

**1. `Pytest`: El Compa√±ero de Pruebas Elegido por Su Simplicidad y Poder üõ°Ô∏è**

Mientras que Python viene con el m√≥dulo `unittest` en su librer√≠a est√°ndar, **`pytest`** se ha convertido en el `framework` de pruebas de facto para muchos en la comunidad Python, y por buenas razones:

* **Menos `Boilerplate`:** Escribir pruebas con `pytest` es a menudo m√°s conciso. No necesitas heredar de clases base para la mayor√≠a de los casos.
* **`Fixtures` Potentes:** Un sistema de `dependency injection` para tus pruebas, que permite configurar `setup/teardown` de forma modular y reutilizable (veremos `fixtures` en 14.9).
* **`Assertions` Simples:** Usa el `assert` est√°ndar de Python, lo que hace las pruebas m√°s legibles. `Pytest` proporciona introspecci√≥n detallada en caso de fallo.
* **Vasto Ecosistema de `Plugins`:** Para `reporting`, `coverage`, pruebas as√≠ncronas, integraci√≥n con otros `frameworks`, etc.
* **Excelente Integraci√≥n con FastAPI:** Funciona de maravilla para probar aplicaciones FastAPI, incluyendo sus aspectos as√≠ncronos.

**2. Configurando Tu Laboratorio de Pruebas: Instalaci√≥n y Convenciones üß™**

Manos a la obra. Para empezar a probar tu aplicaci√≥n FastAPI con `pytest`:

*   **Instalaci√≥n:**\
    Necesitar√°s `pytest` y, dado que FastAPI es as√≠ncrono, `pytest-asyncio` es indispensable. `httpx` es usado por el `TestClient` de FastAPI.

    ```bash
    pip install pytest pytest-asyncio httpx
    ```
*   **Convenciones Fundamentales de `Pytest`:**`Pytest` usa convenciones para descubrir autom√°ticamente tus pruebas:

    | Elemento                          | Convenci√≥n de Nombrado (`Naming Convention`) | Ejemplo (`Example`)                  |
    | --------------------------------- | -------------------------------------------- | ------------------------------------ |
    | Archivos de Test (`Test Files`)   | `test_*.py` o `*_test.py`                    | `test_main_api.py`, `auth_test.py`   |
    | Funciones Test (`Test Functions`) | `test_*()`                                   | `test_read_root_success()`           |
    | Clases Test (`Test Classes`)      | `Test*` (si decides agrupar tests en clases) | `TestUserAuthentication`             |
    | `Assertions`                      | `assert <expresi√≥n_booleana>`                | `assert response.status_code == 200` |

**3. `TestClient` de FastAPI: Tu Cliente Fantasma para Pruebas HTTP üëª**

FastAPI viene con una herramienta incre√≠blemente √∫til para probar tus `endpoints` HTTP: `TestClient`.

* **Importaci√≥n:** `from fastapi.testclient import TestClient`
* **¬øQu√© es?** Es un cliente que te permite hacer peticiones HTTP (como `GET`, `POST`, etc.) a tu aplicaci√≥n FastAPI **directamente en memoria, sin necesidad de levantar un servidor HTTP real** (como Uvicorn). Interact√∫a con tu aplicaci√≥n a trav√©s de la interfaz ASGI.
* **Basado en `httpx`:** Esto significa que soporta operaciones tanto s√≠ncronas como as√≠ncronas de forma transparente para tu aplicaci√≥n FastAPI (que es as√≠ncrona).
*   **Instanciaci√≥n:** Se crea una instancia pas√°ndole tu objeto de aplicaci√≥n FastAPI.

    ```python
    from fastapi import FastAPI
    from fastapi.testclient import TestClient

    app = FastAPI() # Tu aplicaci√≥n FastAPI

    @app.get("/")
    async def read_root():
        return {"message": "Hello, FastAPI!"}

    client = TestClient(app) # Instancia del TestClient asociada a tu app
    ```

**4. Tu Primera Prueba `Pytest` con `TestClient`: ¬°El "Hola Mundo" de la Calidad! üö¶**

Vamos a escribir una prueba simple para el `endpoint` `/` que definimos arriba.

*   **Crear un Archivo de Prueba:** Por ejemplo, `test_main.py` en un directorio `tests/` (o en el mismo directorio de tu `app` para empezar).

    ```python
    # tests/test_main.py
    from fastapi import FastAPI
    from fastapi.testclient import TestClient

    # Puedes definir una app simple aqu√≠ para probar o importar tu app principal
    # Para este ejemplo, definimos una app simple:
    app_for_testing = FastAPI()

    @app_for_testing.get("/")
    async def read_root_for_testing():
        return {"greeting": "Hello from Test"}

    # Crear un cliente para esta app de prueba
    client = TestClient(app_for_testing)


    def test_read_root_endpoint():
        """
        Prueba el endpoint ra√≠z para verificar el status code y el contenido JSON.
        """
        response = client.get("/") # El TestClient hace la petici√≥n
        
        assert response.status_code == 200 # Verificar el status code
        assert response.json() == {"greeting": "Hello from Test"} # Verificar el payload JSON

    def test_non_existent_endpoint():
        response = client.get("/non-existent-path")
        assert response.status_code == 404 # FastAPI devuelve 404 para rutas no encontradas
    ```
*   **Ejecutando `pytest`:**\
    Abre tu terminal en el directorio ra√≠z de tu proyecto (donde est√° tu directorio `tests/` o tu archivo `test_main.py`) y simplemente ejecuta:

    ```bash
    pytest
    ```

    `Pytest` descubrir√° y ejecutar√° autom√°ticamente tus funciones `test_*`. Deber√≠as ver una salida indicando que las pruebas han pasado.

**5. Probando `Endpoints` As√≠ncronos (y Pruebas As√≠ncronas) ‚ö°**

Tu aplicaci√≥n FastAPI es as√≠ncrona. El `TestClient` est√° dise√±ado para trabajar con esto sin problemas, incluso si tu funci√≥n de prueba es s√≠ncrona (como `test_read_root_endpoint` arriba). El `TestClient` maneja el `event loop` internamente para llamar a tu `path operation function` as√≠ncrona.

Sin embargo, si tu **l√≥gica de prueba en s√≠ misma necesita usar `await`** (por ejemplo, para preparar datos con una dependencia as√≠ncrona antes de la llamada HTTP, o para interactuar con `WebSockets` de forma as√≠ncrona en una prueba), entonces tu funci√≥n de prueba debe ser `async def` y debes usar el `marker` de `pytest-asyncio`.

*   **Ejemplo (Ilustrativo, no estrictamente necesario para `GET` simple):**

    ```python
    # tests/test_main.py (continuaci√≥n)
    import pytest # Necesario para el marker @pytest.mark.asyncio

    # ... (app_for_testing y client definidos como antes) ...

    @pytest.mark.asyncio
    async def test_read_root_async_test_function():
        # En este caso particular, la llamada client.get("/") sigue siendo s√≠ncrona en su interfaz,
        # pero el TestClient se encarga de ejecutar la app FastAPI as√≠ncrona.
        # Marcar el test como async es √∫til si necesitas 'await' otras cosas dentro del test.
        
        # async_setup_data = await some_async_setup_function() # Ejemplo de necesidad de await
        
        response = client.get("/") # Esta llamada sigue siendo s√≠ncrona desde la perspectiva del test
        
        assert response.status_code == 200
        assert response.json() == {"greeting": "Hello from Test"}
        
        # await some_async_teardown_function(async_setup_data) # Ejemplo de necesidad de await
    ```

    _Para la mayor√≠a de las pruebas de `endpoints` HTTP simples con `TestClient`, no necesitar√°s marcar tus tests como `@pytest.mark.asyncio` a menos que la l√≥gica de configuraci√≥n o aserci√≥n de tu prueba sea as√≠ncrona. El `TestClient` "puentea" el mundo s√≠ncrono de la prueba con el mundo as√≠ncrono de tu `app` FastAPI para las llamadas HTTP._ Cuando probemos `WebSockets` (14.4), veremos un uso m√°s claro de `@pytest.mark.asyncio`.

**Diagrama `Mermaid`: El Flujo de una Prueba con `TestClient` üåä**

```mermaid
graph TD
    PytestRunner["Pytest Test Runner"] -- Descubre y Ejecuta --> TestFunction["Test Function (e.g., test_read_root)"]
    
    subgraph "Dentro de la Test Function"
        InstantiateApp["Define/Import FastAPI App Object"] --> InstantiateClient["client = TestClient(app)"]
        InstantiateClient -- Petici√≥n HTTP (ej. client.get('/')) --> TestClientLogic["TestClient Logic"]
        TestClientLogic -- Invoca internamente el Path Operation --> FastAPIAppInMemory["FastAPI App (In-Memory)"]
        FastAPIAppInMemory -- Procesa Request y Retorna Datos --> TestClientLogic
        TestClientLogic -- Construye Objeto Response --> ResponseObject["Response Object"]
        ResponseObject -- Entregado a --> Assertions["Assertions en Test Function<br/>(assert response.status_code == ...<br/>assert response.json() == ...)"]
    end
    
    Assertions -- Pasa/Falla --> PytestRunner

    style PytestRunner fill:#87CEEB,stroke:#4682B4
    style TestFunction fill:#D5F5E3,stroke:#2ECC71
    style InstantiateApp fill:#FFF9C4,stroke:#F1C40F
    style InstantiateClient fill:#FFF9C4,stroke:#F1C40F
    style TestClientLogic fill:#FFCDD2,stroke:#C62828
    style FastAPIAppInMemory fill:#E1BEE7,stroke:#8E44AD
    style ResponseObject fill:#FFF9C4
    style Assertions fill:#D5F5E3
```

***

### 14.2 Pruebas unitarias para servicios de dominio

Con nuestro entorno de pruebas FastAPI + `Pytest` listo (14.1), es hora de sumergirnos en el coraz√≥n de nuestra aplicaci√≥n: la **l√≥gica de negocio**. El **14.2** se enfoca en las **pruebas unitarias (`unit tests`) para los servicios de dominio o aplicaci√≥n**. Estas pruebas son fundamentales porque verifican la correcci√≥n de la l√≥gica central de tu sistema de forma aislada, r√°pida y precisa.

Imagina que tus `endpoints` API son la fachada de una f√°brica compleja. Las pruebas unitarias de servicios son como inspeccionar cada m√°quina individual dentro de la f√°brica para asegurar que realiza su tarea espec√≠fica a la perfecci√≥n, sin depender de que el resto de la l√≠nea de ensamblaje est√© funcionando.

***

Mientras que las pruebas con `TestClient` (que veremos m√°s en 14.4) verifican el comportamiento de tus `endpoints` HTTP, las pruebas unitarias para `services` se centran en una capa m√°s profunda: la l√≥gica de negocio que orquesta las operaciones, aplica reglas y toma decisiones. Esta capa es a menudo donde reside la complejidad y el valor real de tu aplicaci√≥n.

**¬øQu√© Son las Pruebas Unitarias para `Services` de Dominio/Aplicaci√≥n? üéØ**

* **Definici√≥n:** Una prueba unitaria verifica la funcionalidad de una "unidad" de c√≥digo de forma **aislada**. En este contexto, una "unidad" suele ser una funci√≥n, un m√©todo dentro de una clase de servicio, o la clase de servicio completa si sus m√©todos colaboran estrechamente para un caso de uso.
* **Objetivos Principales:**
  * Verificar que la l√≥gica interna de la unidad funciona como se espera para diferentes entradas.
  * Asegurar que se manejan correctamente los casos l√≠mite (`edge cases`) y los escenarios de error.
  * Validar que la unidad interact√∫a correctamente con sus dependencias (aunque estas dependencias ser√°n simuladas o `mocked`).
* **El Principio de Aislamiento (CRUCIAL):**\
  Una prueba unitaria **NO** debe interactuar con sistemas externos como:
  * Bases de datos reales.
  * Servicios de red (otras APIs, `message brokers`).
  * El `filesystem`.\
    Cualquier dependencia de este tipo debe ser reemplazada por un **`test double`** (un `mock`, `stub`, o `fake`) durante la prueba.

**Identificando Tus "Unidades" de Prueba: `Services` y L√≥gica de Negocio üîé**

En una arquitectura bien estructurada (como la Hexagonal o una arquitectura por capas), los candidatos ideales para pruebas unitarias son:

* **`Application Services` o `Use Case Handlers`:** Clases que orquestan un caso de uso espec√≠fico, coordinando repositorios, `domain entities`, y otros servicios.
* **`Domain Services`:** Clases que encapsulan l√≥gica de dominio que no encaja naturalmente dentro de una `entity` o `value object`.
* **M√©todos Complejos dentro de `Entities` o `Aggregates`:** A veces, la l√≥gica dentro de tus objetos de dominio es lo suficientemente compleja como para justificar sus propias pruebas unitarias.

La clave es identificar piezas de c√≥digo con l√≥gica de negocio que puedan ser probadas de forma independiente, simulando sus interacciones con el mundo exterior.

**El Arte de la Simulaci√≥n: `Mocking` y `Stubbing` Dependencias con `unittest.mock` üé≠**

Para lograr el aislamiento, necesitamos simular las dependencias de nuestro servicio bajo prueba. La librer√≠a est√°ndar de Python `unittest.mock` (que se integra perfectamente con `pytest`, o puedes usar el `plugin` `pytest-mock` que provee `fixtures` convenientes) es nuestra herramienta para esto.

* **¬øPor Qu√© `Mockear`?**
  * **Aislamiento:** Permite probar la l√≥gica del servicio sin los efectos secundarios o la imprevisibilidad de las dependencias reales.
  * **Velocidad:** Los `mocks` son r√°pidos; no hay latencia de red o BBDD.
  * **Control:** Puedes configurar los `mocks` para que devuelvan valores espec√≠ficos o simulen condiciones de error, permiti√©ndote probar todos los caminos de tu c√≥digo.
  * **Verificaci√≥n de Interacciones:** Puedes verificar que tu servicio llam√≥ a los m√©todos correctos de sus dependencias con los argumentos esperados.
*   **Conceptos Clave de `unittest.mock` (Tabla Resumen):**

    | Atributo/M√©todo del `Mock`                 | Prop√≥sito                                                                                                  | Ejemplo de Uso (`mock_obj.attribute`)                               |
    | ------------------------------------------ | ---------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------- |
    | `return_value`                             | Especifica el valor que un `mock` (o un m√©todo `mockeado`) debe devolver cuando se llama.                  | `mock_repo.get_user.return_value = User(...)`                       |
    | `side_effect`                              | Puede ser una funci√≥n a llamar, una excepci√≥n a lanzar, o una lista de valores a devolver secuencialmente. | `mock_api.call.side_effect = NetworkError("Fallo")`                 |
    | `call_count`                               | Entero que indica cu√°ntas veces se ha llamado al `mock` o a uno de sus atributos.                          | `assert mock_logger.log.call_count == 2`                            |
    | `called`                                   | `Boolean` que indica si el `mock` (o un atributo) fue llamado al menos una vez.                            | `assert mock_notifier.send_email.called is True`                    |
    | `assert_called()`                          | Afirma que el `mock` fue llamado al menos una vez. Lanza `AssertionError` si no.                           | `mock_obj.some_method.assert_called()`                              |
    | `assert_called_once()`                     | Afirma que el `mock` fue llamado exactamente una vez.                                                      | `mock_obj.some_method.assert_called_once()`                         |
    | `assert_called_with(*args, **kwargs)`      | Verifica que la √∫ltima llamada al `mock` fue con los argumentos y `keywords` especificados.                | `mock_repo.save.assert_called_with(user_instance)`                  |
    | `assert_called_once_with(*args, **kwargs)` | Igual, pero asegura que fue llamado exactamente una vez con esos argumentos.                               | `mock_repo.save.assert_called_once_with(user_instance)`             |
    | `assert_any_call(*args, **kwargs)`         | Verifica si el `mock` fue llamado alguna vez con los argumentos especificados.                             | `mock_logger.info.assert_any_call("Proceso iniciado")`              |
    | `call_args`                                | Tupla `(args, kwargs)` de la √∫ltima llamada. O `None`.                                                     | `args, kwargs = mock_obj.some_method.call_args`                     |
    | `call_args_list`                           | Lista de todas las llamadas hechas al `mock`, cada una una tupla `(args, kwargs)`.                         | `for call_obj in mock_obj.some_method.call_args_list: ...`          |
    | `patch` (de `unittest.mock`)               | Un `context manager` o decorador para reemplazar temporalmente objetos en un m√≥dulo con `mocks`.           | `@patch('module.Clase.metodo')` o `with patch(...) as mock_metodo:` |

    * **`AsyncMock`:** Para `mockear` funciones o m√©todos `async`, `unittest.mock.AsyncMock` es esencial. Se comporta como un `MagicMock` pero sus m√©todos `mockeados` devuelven `awaitables` y las `assertions` como `assert_awaited_once_with` est√°n disponibles.
* **Diagrama Conceptual: Prueba Unitaria de un Servicio con `Mocks`**

**Escribiendo Pruebas Unitarias para un `Service`: Un Ejemplo Pr√°ctico üß™**

Imaginemos un `PremiumUserService` que decide si un usuario puede acceder a una `feature` premium bas√°ndose en su tipo de suscripci√≥n y quiz√°s alguna otra l√≥gica.

*   **Definici√≥n del `Service` y sus Dependencias (Interfaces):**

    ```python
    # app/services/premium_service.py
    from typing import Protocol, Optional
    from enum import Enum

    class SubscriptionType(str, Enum):
        FREE = "free"
        PREMIUM = "premium"
        VIP = "vip"

    class User(BaseModel): # Asumimos un Pydantic model para User
        id: int
        username: str
        subscription_type: SubscriptionType

    class IUserRepository(Protocol): # Puerto para el repositorio de usuarios
        async def get_user_by_id(self, user_id: int) -> Optional[User]: ...

    class PremiumFeatureAccessError(Exception):
        pass

    class PremiumUserService:
        def __init__(self, user_repo: IUserRepository):
            self.user_repo = user_repo

        async def can_user_access_feature(self, user_id: int, feature_name: str) -> bool:
            if not feature_name:
                raise ValueError("Feature name cannot be empty.")

            user = await self.user_repo.get_user_by_id(user_id)
            if not user:
                raise ValueError(f"User with ID {user_id} not found.")

            if feature_name == "ultra_hd_streaming":
                if user.subscription_type in [SubscriptionType.PREMIUM, SubscriptionType.VIP]:
                    return True
                else:
                    raise PremiumFeatureAccessError("Ultra HD Streaming only for PREMIUM or VIP users.")
            
            if feature_name == "exclusive_content":
                if user.subscription_type == SubscriptionType.VIP:
                    return True
                else:
                    raise PremiumFeatureAccessError("Exclusive Content only for VIP users.")
            
            # Por defecto, otras features no premium podr√≠an ser accesibles
            return True # O lanzar error si la feature no es conocida
    ```
*   **Escribiendo las Pruebas con `pytest` y `AsyncMock`:**\
    (Aseg√∫rate de tener `pip install pytest pytest-asyncio`)

    ```python
    # tests/unit/services/test_premium_service.py
    import pytest
    from unittest.mock import AsyncMock # Para mockear coroutines
    from pydantic import BaseModel # Necesario para definir User en el test o importar

    # Re-definir o importar las clases necesarias para el test si est√°n en otros m√≥dulos
    # from app.services.premium_service import (
    #     PremiumUserService, IUserRepository, User, SubscriptionType, PremiumFeatureAccessError
    # )

    # --- Definiciones simplificadas para el test (si no se importan) ---
    from enum import Enum
    from typing import Protocol, Optional

    class SubscriptionType(str, Enum):
        FREE = "free"; PREMIUM = "premium"; VIP = "vip"

    class User(BaseModel): 
        id: int; username: str; subscription_type: SubscriptionType

    class IUserRepository(Protocol):
        async def get_user_by_id(self, user_id: int) -> Optional[User]: ...

    class PremiumFeatureAccessError(Exception): pass

    class PremiumUserService: # Copia de la clase para que el test sea autocontenido
        def __init__(self, user_repo: IUserRepository): self.user_repo = user_repo
        async def can_user_access_feature(self, user_id: int, feature_name: str) -> bool:
            if not feature_name: raise ValueError("Feature name cannot be empty.")
            user = await self.user_repo.get_user_by_id(user_id)
            if not user: raise ValueError(f"User with ID {user_id} not found.")
            if feature_name == "ultra_hd_streaming":
                if user.subscription_type in [SubscriptionType.PREMIUM, SubscriptionType.VIP]: return True
                else: raise PremiumFeatureAccessError("Ultra HD Streaming only for PREMIUM or VIP users.")
            if feature_name == "exclusive_content":
                if user.subscription_type == SubscriptionType.VIP: return True
                else: raise PremiumFeatureAccessError("Exclusive Content only for VIP users.")
            return True 
    # --- Fin Definiciones simplificadas ---


    @pytest.fixture
    def mock_user_repo(self) -> AsyncMock: # Usando pytest-mock 'mocker' tambi√©n es una opci√≥n
        return AsyncMock(spec=IUserRepository)

    @pytest.mark.asyncio
    async def test_vip_user_can_access_ultra_hd(mock_user_repo: AsyncMock):
        # Arrange
        vip_user = User(id=1, username="vip_user", subscription_type=SubscriptionType.VIP)
        mock_user_repo.get_user_by_id.return_value = vip_user # Configurar el mock
        
        service = PremiumUserService(user_repo=mock_user_repo)
        
        # Act
        can_access = await service.can_user_access_feature(user_id=1, feature_name="ultra_hd_streaming")
        
        # Assert
        assert can_access is True
        mock_user_repo.get_user_by_id.assert_awaited_once_with(1) # Verificar llamada al mock

    @pytest.mark.asyncio
    async def test_free_user_cannot_access_ultra_hd(mock_user_repo: AsyncMock):
        # Arrange
        free_user = User(id=2, username="free_user", subscription_type=SubscriptionType.FREE)
        mock_user_repo.get_user_by_id.return_value = free_user
        service = PremiumUserService(user_repo=mock_user_repo)
        
        # Act & Assert
        with pytest.raises(PremiumFeatureAccessError) as exc_info:
            await service.can_user_access_feature(user_id=2, feature_name="ultra_hd_streaming")
        
        assert "only for PREMIUM or VIP users" in str(exc_info.value)
        mock_user_repo.get_user_by_id.assert_awaited_once_with(2)

    @pytest.mark.asyncio
    async def test_access_non_existent_user_raises_error(mock_user_repo: AsyncMock):
        # Arrange
        mock_user_repo.get_user_by_id.return_value = None # Simular usuario no encontrado
        service = PremiumUserService(user_repo=mock_user_repo)
        
        # Act & Assert
        with pytest.raises(ValueError) as exc_info:
            await service.can_user_access_feature(user_id=999, feature_name="any_feature")
        
        assert "User with ID 999 not found" in str(exc_info.value)

    # ... m√°s tests para otros casos (ej. feature_name vac√≠o, otras features) ...
    ```

**Estructura de Pruebas: El Patr√≥n `Arrange, Act, Assert (AAA)`**

Una buena pr√°ctica para estructurar tus pruebas unitarias es seguir el patr√≥n AAA:

1. **`Arrange` (Organizar):** Prepara todo lo necesario para la prueba. Esto incluye:
   * Crear instancias de los objetos necesarios.
   * Configurar los `mocks` (establecer `return_value`, `side_effect`, etc.).
   * Definir los `inputs` para la unidad bajo prueba.
2. **`Act` (Actuar):** Ejecuta la unidad de c√≥digo que est√°s probando (ej. llamar al m√©todo del servicio).
3. **`Assert` (Afirmar):** Verifica que los resultados sean los esperados. Esto incluye:
   * Comprobar el valor de retorno de la unidad.
   * Verificar que los `mocks` fueron llamados como se esperaba (`call_count`, `assert_called_with`).
   * Verificar que se lanzaron las excepciones correctas en casos de error.

Los ejemplos anteriores siguen este patr√≥n.

**Beneficios Concretos de las Pruebas Unitarias de `Services` üèÜ**

* **Ejecuci√≥n Extremadamente R√°pida:** Al no depender de BBDD, red, etc., estas pruebas se ejecutan en milisegundos. Esto permite un `feedback loop` muy corto durante el desarrollo.
* **Feedback Preciso y Aislado:** Si una prueba unitaria falla, sabes que el problema est√° en la l√≥gica de esa unidad espec√≠fica, no en una dependencia externa.
* **Fomenta el Buen Dise√±o (SOLID, DI):** Escribir c√≥digo que sea f√°cil de probar unitariamente te empuja naturalmente hacia principios de dise√±o como la `Single Responsibility Principle (SRP)` y la `Dependency Inversion Principle (DIP)` (usando interfaces y `Dependency Injection`).
* **Documentaci√≥n Viva y Ejecutable:** Tus pruebas unitarias sirven como ejemplos concretos de c√≥mo se espera que se comporte cada parte de tu `service`.
* **Seguridad en la Refactorizaci√≥n:** Puedes cambiar la implementaci√≥n interna de un `service` con mucha m√°s confianza si tienes una buena `suite` de pruebas unitarias que verifiquen que su comportamiento observable no ha cambiado.

**Conclusi√≥n: Construyendo Confianza Desde el N√∫cleo de Tu Aplicaci√≥n ‚ù§Ô∏è‚Äçü©π**

Las pruebas unitarias de tus `services` de dominio y aplicaci√≥n son la primera y m√°s fundamental l√≠nea de defensa contra `bugs` y regresiones. Son el microscopio que te permite examinar y validar la l√≥gica m√°s cr√≠tica de tu sistema.`Pytest` y las capacidades de `mocking` de Python (`unittest.mock` o `pytest-mock`) te proporcionan todas las herramientas necesarias para escribir estas pruebas de forma efectiva, aislada y elegante. Esta pr√°ctica no solo mejora la calidad de tu `software`, sino que tambi√©n acelera el desarrollo a largo plazo al proporcionar una red de seguridad para la evoluci√≥n y refactorizaci√≥n de tu c√≥digo.

***

### 14.3 Simulaci√≥n de dependencias con `unittest.mock`

Si en el 14.2 aprendimos _por qu√©_ y _c√≥mo_ probar nuestros `services` de forma aislada conceptualmente, en este **14.3** nos sumergiremos en las herramientas y t√©cnicas espec√≠ficas que nos permiten crear esas "simulaciones" o "dobles de acci√≥n" para las dependencias de nuestro c√≥digo. Nos centraremos en la biblioteca est√°ndar de Python **`unittest.mock`**, con una menci√≥n especial a su integraci√≥n con `pytest` a trav√©s de `pytest-mock`.

Imagina que eres un director de cine (el `test case`) y tu actor principal es el servicio que est√°s probando (`Service Under Test - SUT`). Para que tu actor brille, a veces necesitas que los actores secundarios (las dependencias) digan exactamente sus l√≠neas o se comporten de una manera predecible, incluso si en la vida real son impredecibles (como una API externa o una base de datos). Los `mocks` son tus actores secundarios entrenados para la prueba.

***

Para que las pruebas unitarias sean verdaderamente "unitarias", la unidad de c√≥digo bajo prueba debe estar aislada de sus dependencias externas (bases de datos, APIs de red, `filesystem`, otras clases complejas). La biblioteca `unittest.mock` de Python es el est√°ndar para lograr este aislamiento mediante la creaci√≥n de objetos simulados, conocidos como **`mocks`**.

**`unittest.mock` Profundizado: M√°s All√° de `return_value` üïµÔ∏è‚Äç‚ôÇÔ∏è**

Ya introdujimos `AsyncMock` (una variante de `MagicMock` para `async`) en 14.2. Exploremos un poco m√°s las capacidades de los objetos `Mock` y `MagicMock` (la diferencia principal es que `MagicMock` implementa la mayor√≠a de los m√©todos m√°gicos, haci√©ndolo m√°s transparente como reemplazo):

* **`spec` y `spec_set`: Creando `Mocks` M√°s Seguros y Precisos**\
  Cuando creas un `mock`, puedes pasarle el argumento `spec` (o `spec_set`) con una clase o un objeto como plantilla.
  * `spec=MiClaseReal`: El `mock` solo permitir√° el acceso a atributos y m√©todos que existen en `MiClaseReal`. Si intentas acceder a un atributo inexistente en la `spec`, se lanzar√° un `AttributeError`. Esto ayuda a detectar si tu c√≥digo bajo prueba est√° usando incorrectamente la interfaz de una dependencia (ej. por un `typo` o un cambio en la API de la dependencia).
  * `spec_set=MiClaseReal`: A√∫n m√°s estricto. No solo previene el acceso a atributos no existentes, sino que tambi√©n previene la _asignaci√≥n_ a atributos no existentes en la `spec`.
* **`side_effect` Revisited: Simulando Comportamientos Din√°micos**\
  Mientras `return_value` es est√°tico, `side_effect` es incre√≠blemente vers√°til:
  * **Lanzar Excepciones:** `mock_objeto.metodo.side_effect = MiExcepcion("Error simulado")`. Perfecto para probar c√≥mo tu c√≥digo maneja errores de sus dependencias.
  * **Devolver Diferentes Valores en Llamadas Sucesivas:** `mock_objeto.metodo.side_effect = [10, 20, ValueError("Se acabaron los valores")]`.
  * **Llamar a Otra Funci√≥n:** `mock_objeto.metodo.side_effect = mi_funcion_simuladora`. La funci√≥n `mi_funcion_simuladora` ser√° llamada con los mismos argumentos que el m√©todo `mockeado`.
*   **Configurando `Mocks` Jer√°rquicos (`configure_mock`)**\
    Si necesitas `mockear` un objeto que tiene atributos que a su vez son objetos con m√©todos, puedes configurarlo todo de una vez:

    ```python
    # from unittest.mock import MagicMock
    # mock_user = MagicMock()
    # mock_user.configure_mock(
    #     name="Alice",
    #     address=MagicMock(street="123 Main St", get_zip_code=MagicMock(return_value="90210"))
    # )
    # print(mock_user.name)  # Alice
    # print(mock_user.address.get_zip_code())  # 90210
    ```

**El Poder de `patch`: Reemplazando Dependencias en el Momento Justo üéØ**

A menudo, la dependencia que quieres `mockear` no es algo que inyectas directamente en el constructor de tu SUT, sino algo que tu SUT importa o accede globalmente (ej. `datetime.now()`, una funci√≥n de utilidad en el mismo m√≥dulo, una clase importada de otra librer√≠a). Aqu√≠ es donde `unittest.mock.patch` se convierte en tu herramienta m√°s poderosa.

* **¬øQu√© es `patch`?** Es una funci√≥n que te permite reemplazar temporalmente objetos en un `namespace` espec√≠fico con un `Mock` (o `MagicMock`, `AsyncMock`) durante la ejecuci√≥n de una prueba. Al finalizar la prueba (o el contexto del `patch`), el objeto original se restaura.
* **Formas de Usar `patch`:**
  1.  **Como Decorador (`@patch`):**\
      El `mock object` se inyecta como un argumento en tu funci√≥n de prueba.

      ```python
      # En tu m√≥dulo: my_module.py
      # import datetime
      # def get_current_day_name():
      #     return datetime.datetime.now().strftime('%A')

      # En tu test: tests/test_my_module.py
      from unittest.mock import patch
      # from my_app.my_module import get_current_day_name # Asumiendo que est√° en my_app
      import datetime # Necesario para crear el objeto datetime simulado

      # @patch('my_app.my_module.datetime') # Parchear 'datetime' DENTRO de my_module
      # def test_get_current_day_name_on_monday(mock_datetime):
      #     # Configurar el mock para que datetime.datetime.now() devuelva un lunes espec√≠fico
      #     mock_datetime.datetime.now.return_value = datetime.datetime(2025, 5, 19) # Es un lunes
          
      #     day_name = get_current_day_name()
          
      #     assert day_name == "Monday"
      #     mock_datetime.datetime.now.assert_called_once()
      ```

      * **Orden de Decoradores:** Si usas m√∫ltiples `@patch`, el orden de los argumentos inyectados es inverso al de los decoradores (el √∫ltimo decorador es el primer argumento).
  2.  **Como `Context Manager` (`with patch(...) as mock_name`):**\
      √ötil cuando solo necesitas el `mock` en una porci√≥n de tu prueba.

      ```python
      # from unittest.mock import patch
      # from my_app.my_module import some_function_that_uses_external_api
      # import httpx # Suponiendo que la funci√≥n usa httpx

      # def test_some_function_handles_api_error():
      #     with patch('my_app.my_module.httpx.get') as mock_get: # Parchear httpx.get donde se usa
      #         mock_get.side_effect = httpx.RequestError("Simulated network failure")
              
      #         with pytest.raises(MyApplicationNetworkError): # Asumiendo una excepci√≥n custom
      #             some_function_that_uses_external_api("http://example.com/api/data")
              
      #         mock_get.assert_called_once_with("http://example.com/api/data")
      ```
  3.  **`patch.object(target_object, 'attribute_name', ...)`:**\
      Para `mockear` un m√©todo o atributo en un objeto _ya existente_ o una clase.

      ```python
      # from unittest.mock import patch
      # class RealService:
      #     def get_data(self): return "real_data"
      # service_instance = RealService()

      # def test_with_patch_object():
      #     with patch.object(service_instance, 'get_data', return_value="mocked_data_for_instance") as mock_method:
      #         result = service_instance.get_data()
      #         assert result == "mocked_data_for_instance"
      #         mock_method.assert_called_once()
          
      #     # Fuera del with, service_instance.get_data() vuelve a ser el m√©todo real
      #     assert service_instance.get_data() == "real_data" 
      ```
  4. **"¬øD√≥nde `Patchear`?" (`Where to Patch`) ‚Äì El Punto Cr√≠tico:**\
     Este es el concepto m√°s importante y a menudo confuso de `patch`. Debes `patchear` el objeto **donde es buscado (`looked up`) y usado por tu m√≥dulo bajo prueba**, no necesariamente donde est√° definido originalmente.
     * Si `mi_servicio.py` hace `from utils.calculadora import suma_avanzada`, y quieres probar una funci√≥n en `mi_servicio.py` que usa `suma_avanzada`, debes parchear `'mi_servicio.suma_avanzada'`, no `'utils.calculadora.suma_avanzada'`. Tu SUT (`mi_servicio`) tiene su propia referencia a `suma_avanzada`.
  5. **Diagrama `Mermaid` Conceptual del `Patch`:**

**`pytest-mock`: Simplificando `unittest.mock` con `Fixtures` (El `mocker` Fixture) ‚ú®**

El `plugin` `pytest-mock` (que se instala con `pip install pytest-mock`) proporciona un `fixture` llamado `mocker` que ofrece una API un poco m√°s concisa y orientada a `pytest` para usar las funcionalidades de `unittest.mock`.

*   **Uso del `Fixture`** **`mocker`:**

    ```python
    # tests/test_another_module.py
    # import pytest # No es necesario para el fixture mocker, pero s√≠ para @pytest.mark.asyncio etc.
    # from another_app.another_module import function_using_os_environ

    # def test_function_with_mocker(mocker): # Inyectar el fixture mocker
    #     # Mockear os.environ.get
    #     mock_getenv = mocker.patch('another_app.another_module.os.environ.get')
    #     mock_getenv.return_value = "mocked_api_key"
        
    #     result = function_using_os_environ("API_KEY")
        
    #     assert result == "mocked_api_key"
    #     mock_getenv.assert_called_once_with("API_KEY", None) # Asumiendo que la funci√≥n tiene un default=None
    ```
* `mocker.patch(...)`: Funciona de manera muy similar a `unittest.mock.patch`, pero como es un `fixture`, su "des-parcheo" (restauraci√≥n) se maneja autom√°ticamente al final del `scope` de la prueba (usualmente la funci√≥n).
* `mocker.spy(objeto, 'nombre_metodo')`: Permite "espiar" las llamadas a un m√©todo real sin reemplazar su funcionalidad. √ötil para verificar que un m√©todo real fue llamado, mientras permites que su l√≥gica original se ejecute.
* `mocker.stub(name='mi_stub')`: Crea un objeto `mock` simple y gen√©rico.

**Tabla Comparativa R√°pida: `unittest.mock.patch` vs. `mocker.patch`**

| Caracter√≠stica           | `unittest.mock.patch` (decorador / `context manager`) | `pytest-mock` (`mocker` fixture)                                  |
| ------------------------ | ----------------------------------------------------- | ----------------------------------------------------------------- |
| **Uso Principal**        | Decorador (`@patch`) o `with patch(...)`.             | A trav√©s del `fixture` `mocker` (ej. `mocker.patch(...)`).        |
| **Paso del `Mock`**      | Como argumento a la funci√≥n de test (para decorador). | El m√©todo `patch` de `mocker` devuelve el `mock object`.          |
| **`Scope` / Limpieza**   | Controlado por el alcance del decorador o `with`.     | Autom√°tico al final del `scope` de la prueba (funci√≥n).           |
| **API**                  | API est√°ndar de Python, un poco m√°s verbosa.          | API un poco m√°s concisa, estilo `pytest`.                         |
| **Curva de Aprendizaje** | Parte de la librer√≠a est√°ndar.                        | Requiere el `plugin` `pytest-mock`, pero muy popular en `pytest`. |

**Conclusi√≥n: La Simulaci√≥n como Herramienta Esencial para el Aislamiento y Control Fino üî¨**

Dominar las t√©cnicas de simulaci√≥n con `unittest.mock` (y su conveniente envoltorio `pytest-mock`) es absolutamente fundamental para escribir pruebas unitarias efectivas y significativas.

* Te permite **aislar completamente** la unidad de c√≥digo que est√°s probando.
* Te da **control total** sobre el comportamiento de sus dependencias, permiti√©ndote simular una amplia gama de escenarios (casos de √©xito, errores, valores l√≠mite).
* Resulta en pruebas que son **m√°s r√°pidas, m√°s fiables y m√°s precisas** en la localizaci√≥n de fallos.

Con estas herramientas, puedes construir una red de seguridad robusta alrededor de tu l√≥gica de negocio, permiti√©ndote refactorizar y a√±adir nuevas `features` con mucha mayor confianza.

***

### 14.4 `TestClient` para REST y WebSocket

Ya hemos sentado las bases de nuestro entorno de pruebas con `Pytest` (14.1) y hemos explorado c√≥mo `mockear` dependencias para las pruebas unitarias de nuestros `services` (14.2, 14.3). Ahora, en el **14.4**, volvemos a la "fachada" de nuestra aplicaci√≥n FastAPI: los `endpoints` HTTP RESTful y los canales `WebSocket`. Vamos a ver c√≥mo el `TestClient` de FastAPI nos permite probar ambos de manera efectiva.

***

El `TestClient` de FastAPI (`from fastapi.testclient import TestClient`) es una herramienta incre√≠blemente vers√°til. No solo nos permite simular peticiones HTTP a nuestros `endpoints` RESTful (como vimos brevemente en 14.1), sino que tambi√©n proporciona una forma de interactuar y probar nuestros `endpoints` `WebSocket`.

**1. `TestClient` para `Endpoints` RESTful (Un Repaso Reforzado)**

Recordemos que para probar `endpoints` HTTP, instanciamos el `TestClient` con nuestra `app` FastAPI y luego usamos m√©todos que se asemejan a los de librer√≠as HTTP como `requests`:

```python
from fastapi import FastAPI
from fastapi.testclient import TestClient
from pydantic import BaseModel

# --- Aplicaci√≥n FastAPI de Ejemplo ---
class Item(BaseModel):
    id: int
    name: str
    description: Optional[str] = None

fake_db_items = {
    1: {"id": 1, "name": "Manzana", "description": "Roja y jugosa"},
    2: {"id": 2, "name": "Pl√°tano", "description": "Amarillo y curvo"}
}

app = FastAPI()

@app.get("/items/{item_id}", response_model=Item)
async def read_item(item_id: int):
    if item_id not in fake_db_items:
        raise HTTPException(status_code=404, detail="Item not found")
    return fake_db_items[item_id]

@app.post("/items/", response_model=Item, status_code=201)
async def create_item(item: Item): # Asumimos que el ID viene en el payload para este ejemplo simple
    if item.id in fake_db_items:
        raise HTTPException(status_code=400, detail="Item already exists")
    fake_db_items[item.id] = item.model_dump()
    return item

# --- Pruebas con TestClient ---
client = TestClient(app) # Instanciar una vez, puede ser un fixture en pytest

def test_read_existing_item():
    response = client.get("/items/1")
    assert response.status_code == 200
    assert response.json() == {"id": 1, "name": "Manzana", "description": "Roja y jugosa"}

def test_read_non_existing_item():
    response = client.get("/items/99")
    assert response.status_code == 404
    assert response.json() == {"detail": "Item not found"}

def test_create_item_success():
    new_item_data = {"id": 3, "name": "Naranja", "description": "C√≠trica y dulce"}
    response = client.post("/items/", json=new_item_data) # Enviar payload JSON
    assert response.status_code == 201
    assert response.json() == new_item_data
    # Verificar que realmente se a√±adi√≥ (opcional, depende de si quieres probar estado)
    assert 3 in fake_db_items 
    assert fake_db_items[3]["name"] == "Naranja"

def test_create_existing_item_fails():
    existing_item_data = {"id": 1, "name": "Manzana Otra Vez", "description": "Ya existe"}
    response = client.post("/items/", json=existing_item_data)
    assert response.status_code == 400 # O 409 Conflict, seg√∫n tu l√≥gica
    assert response.json() == {"detail": "Item already exists"}
```

* **Puntos Clave para Pruebas REST con `TestClient`:**
  * Usar `client.get()`, `client.post()`, `client.put()`, `client.patch()`, `client.delete()`.
  * Pasar `payloads` JSON con el argumento `json=...`.
  * Pasar `query parameters` con el argumento `params=...`.
  * Pasar `headers` con el argumento `headers=...`.
  * Verificar `response.status_code` y `response.json()` (o `response.text`, `response.content`).

**2. `TestClient` para `Endpoints`** **`WebSocket`: Estableciendo la Conexi√≥n de Prueba**

Probar `WebSockets` es un poco diferente porque las conexiones son persistentes y bidireccionales. El `TestClient` de FastAPI nos permite simular este comportamiento.

*   **Entrando en el Contexto `WebSocket`:**\
    El `TestClient` tiene un m√©todo `websocket_connect(url, subprotocols=None, ...)` que se usa como un `context manager` (`with` statement). Esto establece la conexi√≥n `WebSocket` simulada y te da un objeto `websocket` (similar al que recibes en tu `endpoint` FastAPI) para interactuar.

    ```python
    # from fastapi.testclient import TestClient
    # from fastapi import WebSocket # Solo para type hinting si es necesario

    # app = FastAPI() # Tu app con endpoints WebSocket
    # client = TestClient(app)

    # def test_websocket_connection_and_echo():
    #     with client.websocket_connect("/ws/echo") as websocket: # websocket es un Starlette WebSocketTestSession
    #         # La conexi√≥n est√° establecida aqu√≠ (equivalente a await websocket.accept() en el server)
    #         # ... pruebas de env√≠o y recepci√≥n ...
    #         pass # Al salir del 'with', la conexi√≥n se cierra
    ```

    * **Importante:** Cuando usas `client.websocket_connect()`, el `TestClient` se encarga de simular el `handshake` y llama internamente al `await websocket.accept()` en tu `endpoint` del `server`. No necesitas (y no debes) llamar a `accept()` en el objeto `websocket` que te da `websocket_connect()`.
* **Enviando y Recibiendo Mensajes `WebSocket` en la Prueba:**\
  El objeto `websocket` devuelto por `websocket_connect()` tiene m√©todos para enviar y recibir datos:
  * `websocket.send_text(data)`
  * `websocket.send_bytes(data)`
  * `websocket.send_json(data)`
  * `websocket.receive_text() -> str`
  * `websocket.receive_bytes() -> bytes`
  * `websocket.receive_json() -> Any`
  * `websocket.close(code=1000)`
*   **Ejemplo Pr√°ctico: Probando un `Endpoint`** **`WebSocket` Eco Simple**

    Supongamos este `endpoint` `WebSocket` en nuestra `app` FastAPI:

    ```python
    # En tu main_app.py
    # from fastapi import FastAPI, WebSocket, WebSocketDisconnect

    # app = FastAPI() # Ya definida si es el mismo archivo que los tests REST

    @app.websocket("/ws/simple_echo")
    async def websocket_simple_echo_endpoint(websocket: WebSocket):
        await websocket.accept()
        try:
            while True:
                data = await websocket.receive_text()
                await websocket.send_text(f"Echo: {data}")
        except WebSocketDisconnect:
            print(f"Cliente {websocket.client.host}:{websocket.client.port} desconectado de Simple Echo.")
    ```

    Ahora, la prueba:

    ```python
    # tests/test_websockets.py
    # from fastapi.testclient import TestClient
    # from main_app import app # Importa tu app FastAPI

    # client = TestClient(app) # Puedes definirlo globalmente en el test file o como fixture

    def test_simple_echo_websocket():
        with client.websocket_connect("/ws/simple_echo") as websocket:
            # Enviar un mensaje al server
            test_message = "Hola WebSocket!"
            websocket.send_text(test_message)
            
            # Recibir la respuesta del server
            response_message = websocket.receive_text()
            assert response_message == f"Echo: {test_message}"

            # Enviar otro mensaje
            websocket.send_text("¬øFunciona de nuevo?")
            response_message_2 = websocket.receive_text()
            assert response_message_2 == "Echo: ¬øFunciona de nuevo?"
            
            # Opcional: cerrar la conexi√≥n desde el cliente de prueba
            # websocket.close(code=1000) 
            # Si no se cierra expl√≠citamente, se cierra al salir del 'with'
    ```
*   **Probando `JSON Payloads`:**

    ```python
    # @app.websocket("/ws/json_echo")
    # async def websocket_json_echo(websocket: WebSocket):
    #     await websocket.accept()
    #     try:
    #         while True:
    #             data = await websocket.receive_json()
    #             await websocket.send_json({"response_to": data, "processed": True})
    #     except WebSocketDisconnect:
    #         pass

    # def test_json_echo_websocket():
    #     with client.websocket_connect("/ws/json_echo") as websocket:
    #         test_payload = {"client_id": 123, "message": "Datos en JSON"}
    #         websocket.send_json(test_payload)
            
    #         response_payload = websocket.receive_json()
    #         assert response_payload == {"response_to": test_payload, "processed": True}
    ```

**3. Manejando la Asincron√≠a en Pruebas `WebSocket` con `pytest-asyncio`**

Si bien los m√©todos del `websocket` del `TestClient` (`send_text`, `receive_text`, etc.) son s√≠ncronos en su interfaz, el `endpoint` `WebSocket` de tu `app` FastAPI es as√≠ncrono. El `TestClient` maneja esto.

Sin embargo, si tu l√≥gica de prueba _alrededor_ de la interacci√≥n `WebSocket` necesita ser as√≠ncrona (ej. necesitas `await` para configurar algo antes de conectar, o para verificar un efecto secundario as√≠ncrono despu√©s de un `message`), tu funci√≥n de prueba debe ser `async def` y usar `@pytest.mark.asyncio`.

*   **En general, para probar `WebSockets`, donde la interacci√≥n es inherentemente un flujo de `messages` que puede necesitar esperas o `delays` simulados, es m√°s natural y a veces necesario escribir las pruebas `WebSocket` como `async`:**

    ```python
    import pytest
    import asyncio # Para asyncio.sleep si es necesario simular delays
    # from fastapi.testclient import TestClient
    # from main_app import app 
    # client = TestClient(app)

    # @pytest.mark.asyncio
    # async def test_websocket_with_server_push_and_async_test_logic(client: TestClient): # Pasando client como fixture
        # Supongamos un endpoint que env√≠a un mensaje despu√©s de un delay
        # @app.websocket("/ws/delayed_push")
        # async def ws_delayed_push(websocket: WebSocket):
        #     await websocket.accept()
        #     await websocket.send_text("Conectado. Espera el push...")
        #     await asyncio.sleep(0.1) # Simular trabajo en backend
        #     await websocket.send_text("¬°Aqu√≠ est√° tu push!")

        # with client.websocket_connect("/ws/delayed_push") as websocket:
        #     initial_message = websocket.receive_text() # Esto es s√≠ncrono
        #     assert initial_message == "Conectado. Espera el push..."
            
        #     # Si el 'TestClient's websocket.receive_text()' tuviera un timeout y
        #     # quisi√©ramos controlarlo con asyncio.wait_for, o si la l√≥gica de aserci√≥n
        #     # dependiera de otros awaitables, el test async ser√≠a m√°s claro.
        #     # Starlette's WebSocketTestSession.receive_text() puede tener un timeout.
        #     try:
        #         pushed_message = websocket.receive_text(timeout=0.5) # Usar el timeout del m√©todo
        #         assert pushed_message == "¬°Aqu√≠ est√° tu push!"
        #     except TimeoutError: # (O la excepci√≥n espec√≠fica que lance por timeout)
        #         pytest.fail("El servidor no hizo push del mensaje a tiempo.")
    ```

    * **Nota Importante sobre `TestClient` y `async` tests:** El `TestClient` en s√≠ mismo no es un cliente `async` que usar√≠as con `await client.get()`. Sus m√©todos (`.get()`, `.post()`, `.websocket_connect()`) son s√≠ncronos. El `TestClient` internamente maneja la ejecuci√≥n del `event loop` de tu `app` `async` FastAPI. Si tu _test function_ necesita ser `async def` (para usar `await` para otras cosas, como `asyncio.sleep` o `async fixtures`), entonces usas `@pytest.mark.asyncio`. El objeto `websocket` que obtienes de `client.websocket_connect()` tambi√©n tiene una interfaz s√≠ncrona (`websocket.send_text()`, `websocket.receive_text()`).

**4. Diagrama `Mermaid`: Flujo de Prueba `WebSocket` con `TestClient`**

```mermaid
graph TD
    PytestRunner["Pytest Test Runner"] -- Ejecuta --> TestWSFunc["Test Function (test_websocket_echo)"]
    
    subgraph "Dentro de la Test Function"
        InstClient["client = TestClient(app)"]
        WS_ConnectContext["with client.websocket_connect('/ws') as ws_session:"]
        
        subgraph "Contexto Conectado"
            SendMsg["ws_session.send_text('Hola')"]
            ReceiveMsg["data = ws_session.receive_text()"]
            Assertions["assert data == 'Echo: Hola'"]
        end
    end
    
    WS_ConnectContext -- Establece Conexi√≥n Simulada --> FastAPIApp_WS_EP["FastAPI App WebSocket Endpoint<br>(@app.websocket('/ws'))"]
    FastAPIApp_WS_EP -- Dentro del Endpoint --> Accept["await websocket.accept()"]
    SendMsg -- Mensaje --> FastAPIApp_WS_EP
    FastAPIApp_WS_EP -- Procesa y Env√≠a Eco --> ReceiveMsg
    
    TestWSFunc -- Pasa/Falla --> PytestRunner

    style PytestRunner fill:#87CEEB,stroke:#4682B4
    style TestWSFunc fill:#D5F5E3,stroke:#2ECC71
    style InstClient fill:#FFF9C4,stroke:#F1C40F
    style WS_ConnectContext fill:#FFCDD2,stroke:#C62828
    style FastAPIApp_WS_EP fill:#E1BEE7,stroke:#8E44AD
    style Assertions fill:#D5F5E3
```

**Conclusi√≥n: Simulando Conversaciones en Tiempo Real para una Calidad Asegurada üí¨**

El `TestClient` de FastAPI es una herramienta unificada y poderosa que te permite probar rigurosamente tanto tus `endpoints` RESTful como tus canales `WebSocket` sin la complejidad de levantar un servidor real o de `mockear` toda la pila de red.

* Para **REST**, proporciona una interfaz familiar similar a `requests`.
* Para **`WebSockets`**, su `context manager` `websocket_connect()` y los m√©todos de env√≠o/recepci√≥n en la `session` de prueba te dan un control preciso para simular interacciones `client-server` completas.

Al integrar estas pruebas en tu flujo de trabajo, puedes construir aplicaciones FastAPI con interfaces de comunicaci√≥n en tiempo real que no solo son ricas en `features`, sino tambi√©n robustas, fiables y menos propensas a regresiones.

***

### 14.5 Pruebas de integraci√≥n con DB temporal

Hemos sentado las bases con `Pytest` (14.1) y hemos visto c√≥mo aislar y probar nuestra l√≥gica de negocio con `unit tests` y `mocks` (14.2, 14.3). Tambi√©n sabemos c√≥mo probar nuestros `endpoints` HTTP y `WebSocket` usando el `TestClient` (14.4), pero hasta ahora, esas pruebas de `endpoints` no interactuaban con una base de datos real.

Es hora de dar un paso m√°s y realizar **pruebas de integraci√≥n (`integration tests`)**, donde verificamos que nuestra aplicaci√≥n FastAPI interact√∫a correctamente con una base de datos, aunque sea una temporal o de prueba. Esto es "la prueba de fuego" para nuestra capa de persistencia.

***

Las pruebas unitarias son fant√°sticas para verificar la l√≥gica aislada, pero no nos dicen si nuestro c√≥digo puede realmente guardar y recuperar datos de una base de datos, si nuestros `mappings` ORM son correctos, o si nuestras `queries` se comportan como esperamos contra un sistema de base de datos real (o casi real). Aqu√≠ es donde las **pruebas de integraci√≥n con una `database` temporal** se vuelven cruciales.

**¬øQu√© Son las Pruebas de Integraci√≥n con Base de Datos? üîó**

* **Foco:** Probar la interacci√≥n entre m√∫ltiples componentes de tu aplicaci√≥n, espec√≠ficamente la capa de aplicaci√≥n (tus `endpoints` FastAPI, `services`, `repositories`) y la capa de persistencia (SQLAlchemy/`Motor` interactuando con una `database instance`).
* **Objetivo:**
  * Verificar que los datos se persisten correctamente.
  * Asegurar que las `queries` recuperan los datos esperados.
  * Validar que los `ORM mappings` (ej. de tus clases SQLAlchemy a tablas) son correctos.
  * Confirmar que la l√≥gica de negocio que depende de la persistencia funciona como se espera.
* **Clave:** En estas pruebas, **no `mockeamos` la base de datos en s√≠ misma ni el `driver`/ORM a bajo nivel**. Interactuamos con una instancia de base de datos real, aunque sea una creada y destruida espec√≠ficamente para las pruebas.

**La Arena de Pruebas: Estrategias para `Databases` Temporales/Aisladas üõ°Ô∏è**

**¬°Regla de Oro: NUNCA uses tu base de datos de desarrollo o producci√≥n para ejecutar pruebas automatizadas!** Esto puede llevar a la corrupci√≥n de datos, resultados de prueba no reproducibles y, en general, al caos. Necesitamos un entorno de base de datos aislado y controlado para nuestras pruebas.

1. **SQLite `In-Memory` (Para SQLAlchemy - R√°pido y F√°cil para Empezar):**
   * **Configuraci√≥n:** Simplemente cambias tu `DATABASE_URL` a algo como `"sqlite+aiosqlite:///:memory:"`.
   * **Pros:**
     * Extremadamente r√°pido para ejecutar pruebas, ya que opera en memoria.
     * No requiere `setup` de un `server` de base de datos externo.
     * Ideal para `CI pipelines` r√°pidos o desarrollo local.
   * **Cons:**
     * **No es tu `database` de producci√≥n:** SQLite tiene su propio dialecto SQL y no soporta todas las `features` o `constraints` de bases de datos m√°s robustas como PostgreSQL o MySQL (ej. ciertos tipos de `ALTER TABLE`, `schemas` complejos, `window functions`, algunos `constraints` avanzados).
     * **Riesgo de Falsos Positivos/Negativos:** Una prueba podr√≠a pasar en SQLite pero fallar en PostgreSQL (o viceversa) debido a estas diferencias.
2. **Instancia de `Database` de Prueba en Docker (M√°xima Fidelidad):**
   * **Configuraci√≥n:** Usar Docker para levantar una instancia contenerizada de la misma tecnolog√≠a de base de datos que usas en producci√≥n (ej. una imagen oficial de `postgres`, `mysql`, o `mongo`).
   * **`pytest-docker` o `Fixtures` Personalizados:** `Pytest plugins` como `pytest-docker` o `fixtures` personalizados pueden gestionar el ciclo de vida de estos contenedores (iniciarlos antes de la `test suite`, detenerlos despu√©s).
   * **Pros:**
     * **Alta Fidelidad:** Est√°s probando contra el mismo motor de base de datos que usar√°s en producci√≥n, lo que da mucha m√°s confianza.
   * **Cons:**
     * M√°s lento de iniciar y configurar que SQLite `in-memory`.
     * Requiere que Docker est√© disponible en el entorno de pruebas.
3. **`Database` de Prueba Dedicada o `Schemas` Temporales (En un `Server` Compartido):**
   * **Configuraci√≥n:** Te conectas a un `server` de base de datos de desarrollo/pruebas existente, pero usas un nombre de base de datos completamente separado para tus pruebas (ej. `mi_app_test_db`) o, si tu BBDD lo soporta bien (como PostgreSQL), creas un `schema` nuevo para cada `test run` (o `test session`) y lo destruyes al final.
   * **Pros:** Puede ser m√°s r√°pido que levantar un Docker nuevo cada vez si el `server` ya est√° corriendo. Permite usar `features` completas de la BBDD.
   * **Cons:** Requiere una gesti√≥n cuidadosa para asegurar el aislamiento entre `test runs` y para evitar conflictos si m√∫ltiples desarrolladores o `CI jobs` comparten el `server`.

**Tabla Comparativa de Estrategias de `Test Database`:**

| Estrategia                 | Pros                                                                                            | Contras                                                                                                 | Ideal Para...                                                                        |
| -------------------------- | ----------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------ |
| SQLite `In-Memory`         | Muy r√°pido, sin `setup` externo, bueno para `CI` b√°sicos.                                       | No es igual a BBDD de producci√≥n, `feature set` limitado, riesgo de falsos positivos/negativos.         | Pruebas unitarias de `repositories` (si abstraen bien el SQL), `prototyping` r√°pido. |
| Dockerized `Test DB`       | M√°xima fidelidad (misma tecnolog√≠a que producci√≥n), aislamiento total.                          | M√°s lento de iniciar, requiere Docker, puede consumir m√°s recursos.                                     | Pruebas de integraci√≥n exhaustivas, `CI pipelines` que buscan la m√°xima confianza.   |
| `Dedicated Test DB/Schema` | Usa `features` completas de la BBDD, puede ser m√°s r√°pido que Docker si el `server` est√° listo. | Requiere gesti√≥n cuidadosa del aislamiento y limpieza, puede haber contenci√≥n en `servers` compartidos. | Entornos de desarrollo controlados, equipos con infraestructura de BBDD de prueba.   |

**`Pytest Fixtures`: Orquestando el Ciclo de Vida de Tu `Test Database` ü™Ñ**

Las `fixtures` de `Pytest` son la herramienta perfecta para gestionar la configuraci√≥n (`setup`) y limpieza (`teardown`) de tu `test database` y las `sessions`.

1.  **`Fixture` para el `Engine` y Creaci√≥n de Tablas (SQLAlchemy):**\
    Suele tener un `scope` de `"session"` (se ejecuta una vez por toda la sesi√≥n de pruebas) o `"module"`.

    ```python
    # conftest.py (o en tu archivo de test principal)
    import pytest_asyncio # Para fixtures async
    from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
    from sqlalchemy.orm import sessionmaker
    # from app.db.base_class import Base # Tu Base de SQLAlchemy
    # from app.core.config import settings # Para la URL de la BBDD de test

    # TEST_SQLALCHEMY_DATABASE_URL = "sqlite+aiosqlite:///:memory:" # O tu URL de test Dockerizada

    # @pytest_asyncio.fixture(scope="session") # O "module"
    # async def test_engine():
    #     engine = create_async_engine(TEST_SQLALCHEMY_DATABASE_URL, echo=False, future=True)
    #     async with engine.begin() as conn:
    #         await conn.run_sync(Base.metadata.drop_all) # Limpiar primero
    #         await conn.run_sync(Base.metadata.create_all) # Crear tablas
    #     yield engine # Proveer el engine
    #     await engine.dispose() # Cerrar conexiones del engine al final de la sesi√≥n
    ```
2.  **`Fixture` para la `Test Session` (SQLAlchemy):**\
    Usualmente con `scope="function"` (cada funci√≥n de test obtiene una `session` nueva y limpia). Una estrategia com√∫n es envolver cada test en una transacci√≥n que se hace `rollback` al final.

    ```python
    # conftest.py (continuaci√≥n)
    # from sqlalchemy.ext.asyncio import async_sessionmaker # O la factor√≠a que definiste

    # TestAsyncSessionLocal = async_sessionmaker(
    #     autocommit=False, autoflush=False, bind=test_engine, class_=AsyncSession, expire_on_commit=False
    # ) # Asumiendo que test_engine es un fixture

    # @pytest_asyncio.fixture(scope="function")
    # async def db_test_session(test_engine) -> AsyncGenerator[AsyncSession, None]: # Depende del engine fixture
    #     # Crear la factor√≠a de sesiones dentro del fixture si el engine es de sesi√≥n tambi√©n
    #     TestAsyncSessionLocal = async_sessionmaker(bind=test_engine, class_=AsyncSession, expire_on_commit=False)
        
    #     async with TestAsyncSessionLocal() as session:
    #         # Opcional: Iniciar una transacci√≥n anidada que se pueda hacer rollback f√°cilmente
    #         # await session.begin_nested() # O manejar la transacci√≥n principal
    #         # Esta es una forma m√°s com√∫n de asegurar el aislamiento:
    #         connection = await test_engine.connect()
    #         transaction = await connection.begin()
    #         session_for_test = AsyncSession(bind=connection, expire_on_commit=False)
            
    #         yield session_for_test # Tu test usa esta session

    #         # Limpieza: cerrar sesi√≥n y hacer rollback de la transacci√≥n
    #         await session_for_test.close()
    #         if transaction.is_active:
    #             await transaction.rollback()
    #         await connection.close()
    ```
3.  **Sobrescribiendo Dependencias de FastAPI (`app.dependency_overrides`):**\
    Este es el pegamento que hace que tu `app` FastAPI use la `test database session` durante las pruebas.

    ```python
    # conftest.py (continuaci√≥n)
    # from fastapi import FastAPI
    # from app.main import app as main_app # Tu app FastAPI principal
    # from app.api.dependencies import get_session as get_production_session # Tu dep de producci√≥n

    # @pytest_asyncio.fixture(scope="module") # O "session"
    # def test_app(test_engine) -> FastAPI: # Asumiendo que test_engine ya crea tablas
    #     # Aqu√≠ sobreescribimos la dependencia 'get_session' para que use la BBDD de test
    #     # Es crucial que 'get_test_session_override' sea una factor√≠a que pueda ser llamada por Depends
        
    #     async def get_test_session_override() -> AsyncGenerator[AsyncSession, None]:
    #         TestAsyncSessionLocal = async_sessionmaker(bind=test_engine, class_=AsyncSession, expire_on_commit=False)
    #         async with TestAsyncSessionLocal() as session:
    #             yield session
        
    #     main_app.dependency_overrides[get_production_session] = get_test_session_override
    #     return main_app

    # @pytest_asyncio.fixture(scope="function")
    # async def client(test_app: FastAPI) -> TestClient: # Ahora el TestClient usa la app con la BBDD de test
    #     with TestClient(test_app) as c:
    #         yield c
    ```

    _En tu test, ahora usar√≠as el `fixture`_ _`client` y el `fixture`_ _`db_test_session` (si necesitas hacer `asserts` directamente en la BBDD)._

**Escribiendo Pruebas de Integraci√≥n: El `TestClient` se Encuentra con la `Database` Real ü§ù**

1. **`Arrange` (Organizar):**
   * Las `fixtures` de `Pytest` configuran la `test database` y la `test session`.
   * `app.dependency_overrides` asegura que la `app` usa esta `test session`.
   * (Opcional) Tu test puede insertar datos de pre-condici√≥n directamente en la `test database` usando la `test session fixture`.
2. **`Act` (Actuar):**
   * Usas el `TestClient` (que ahora est√° configurado para usar la `test database`) para hacer una petici√≥n HTTP a un `endpoint` FastAPI que realiza operaciones de base deatos.
3. **`Assert` (Afirmar):**
   * Verifica la respuesta HTTP (`status_code`, `json()`).
   * **Y crucialmente:** Usa la `test session fixture` para hacer una `query` directa a la `test database` y verificar que el estado de los datos es el esperado (ej. el `item` fue creado, actualizado, eliminado correctamente).

*   **Ejemplo Pr√°ctico (FastAPI + SQLAlchemy, usando los `fixtures` anteriores):**

    ```python
    # tests/integration/test_product_api_integration.py
    # import pytest
    # from fastapi import status
    # from httpx import AsyncClient # TestClient usa httpx
    # from sqlalchemy.ext.asyncio import AsyncSession
    # from sqlalchemy import select

    # from app.models.product_db import ProductDB # Tu modelo ORM
    # from app.schemas.product_api import ProductCreatePydantic # Tu DTO

    # @pytest.mark.asyncio
    # async def test_create_product_integration(client: AsyncClient, db_test_session: AsyncSession): # client ahora es AsyncClient
    #     # Nota: TestClient s√≠ncrono tambi√©n funciona, pero AsyncClient es para cuando el test es async
    #     # Para usar AsyncClient, necesitas instanciarlo con la app y un base_url
    #     # from main_app import app # tu app FastAPI
    #     # async with AsyncClient(app=app, base_url="http://testserver") as ac:
    #     #     response = await ac.post("/products/", json={"name": "Test Product Int", "price": 19.99, "is_available": True})

    #     # Simplificando con el TestClient s√≠ncrono que ya configuramos en conftest.py con overrides
    #     # from fastapi.testclient import TestClient
    #     # client_sync = TestClient(test_app_fixture) # test_app_fixture es la app con dependencias sobrescritas
        
    #     product_data = {"name": "Super Test Product", "description": "Integration Test", "price": 99.99, "is_available": True}
        
    #     # Act
    #     response = client.post("/api/v1/products/", json=product_data) # Asumiendo que 'client' es el fixture de TestClient s√≠ncrono

    #     # Assert HTTP Response
    #     assert response.status_code == status.HTTP_201_CREATED
    #     response_data = response.json()
    #     assert response_data["name"] == product_data["name"]
    #     assert "id" in response_data
    #     product_id = response_data["id"]

    #     # Assert Database State
    #     # db_test_session es la session que el endpoint us√≥ (debido al override)
    #     # y que podemos usar aqu√≠ para verificar, ANTES de que el fixture haga rollback.
    #     # Para verificar que el commit sucedi√≥ dentro del endpoint:
    #     # (Esto requiere que el endpoint haga commit y la fixture de sesi√≥n no haga rollback inmediato,
    #     # o que uses una sesi√≥n separada para verificar si la fixture de sesi√≥n hace rollback.)

    #     # Si la fixture db_test_session hace rollback al final, necesitamos consultar *antes* de que termine el test.
    #     # La sesi√≥n usada por el endpoint ES la misma que db_test_session.
    #     # El commit se hizo en el endpoint. Ahora verificamos.
    #     persisted_product = await db_test_session.get(ProductDB, product_id)
        
    #     assert persisted_product is not None
    #     assert persisted_product.name == product_data["name"]
    #     assert persisted_product.price == product_data["price"]
        
    #     # La fixture se encargar√° del rollback despu√©s de que este test termine,
    #     # aislando este test de otros.
    ```

**Diagrama `Mermaid`: Flujo de una Prueba de Integraci√≥n con `Database` Temporal y `Dependency Override`**

```mermaid
graph TD
    PytestRunner["Pytest Runner"]

    subgraph "Test Setup (Pytest Fixtures)"
        F_Engine["Fixture test_engine<br/>(Crea Engine + Tablas una vez)"]
        F_Session["Fixture db_test_session(test_engine)<br/>(Transacci√≥n + yield session + Rollback)"]
        F_App["Fixture test_app(test_engine)<br/>(app.dependency_overrides[get_prod_session] = test_session_provider)"]
        F_Client["Fixture client(test_app)<br/>(TestClient con app parcheada)"]
    end
    
    PytestRunner -- Ejecuta --> TestFunc["Test Function(client, db_test_session)"]
    
    TestFunc -- Usa --> F_Client
    TestFunc -- Usa (para asserts en DB) --> F_Session
    
    subgraph "Test Execution"
        TestFunc -- client.post(...) --> TestClientObj[TestClient Instance]
        TestClientObj -- Llama a --> FastAPI_App_Patched["FastAPI App (dependency override activo)"]
        FastAPI_App_Patched -- Usa `session` de `db_test_session` --> Test_DB[(Test Database<br/>(ej. SQLite :memory: o Dockerized PostgreSQL))]
        Test_DB -- Datos --> FastAPI_App_Patched
        FastAPI_App_Patched -- HTTP Response --> TestClientObj
        TestClientObj -- `Response` --> TestFunc
    end

    TestFunc -- `assert response.status_code` --> TestFunc
    TestFunc -- `await db_test_session.get(...)` para verificar --> Test_DB
    Test_DB -- Datos para `assert` --> TestFunc
    TestFunc -- `assert db_state` --> TestFunc
    
    Note over F_Session: Al final del test, `db_test_session` hace `ROLLBACK`

    style F_Engine fill:#E1BEE7,stroke:#8E44AD
    style F_Session fill:#E1BEE7,stroke:#8E44AD
    style F_App fill:#E1BEE7,stroke:#8E44AD
    style F_Client fill:#E1BEE7,stroke:#8E44AD
    style TestFunc fill:#D5F5E3,stroke:#2ECC71
    style Test_DB fill:#AED6F1,stroke:#3498DB
```

**Conclusi√≥n: Validando la Danza Completa entre Tu L√≥gica de Aplicaci√≥n y Tus Datos üíÉüï∫**

Las pruebas de integraci√≥n con una `database` temporal son un paso esencial m√°s all√° de los `unit tests`. Te dan la confianza de que:

* Tu l√≥gica de aplicaci√≥n interact√∫a correctamente con la capa de persistencia.
* Tus `mappings` ORM y `queries` se comportan como esperas contra un sistema de base de datos real (o uno muy similar).
* Las transacciones b√°sicas (a nivel de `endpoint` o `service`) funcionan.

`Pytest fixtures` son la clave para gestionar elegantemente el ciclo de vida de tu `test database` (creaci√≥n, `setup` de tablas, provisi√≥n de `sessions`, limpieza/`rollback`). La capacidad de FastAPI para sobrescribir dependencias (`app.dependency_overrides`) es lo que permite que tu aplicaci√≥n utilice esta `test database` de forma transparente durante las pruebas. Aunque son m√°s lentas de ejecutar que los `unit tests`, las pruebas de integraci√≥n de base de datos son invaluables para detectar una clase diferente de `bugs` y asegurar la integridad de tus flujos de datos.

***

### 14.6 Pruebas E2E entre microservicios

Ya hemos explorado las pruebas unitarias (14.2) y de integraci√≥n con bases de datos temporales (14.5). Ahora, subimos un pelda√±o m√°s en la "pir√°mide de testing" para abordar las **pruebas `End-to-End` (`E2E`) entre `microservices`**. Estas son las pruebas que nos dan la m√°xima confianza en que nuestro sistema completo, con todas sus piezas m√≥viles interconectadas, funciona como una sinfon√≠a bien orquestada.

Imagina que las pruebas unitarias verifican cada instrumento musical por separado, y las pruebas de integraci√≥n aseguran que secciones de la orquesta (como los violines y las violas) suenan bien juntas. Las pruebas `E2E` son el ensayo general, con todos los m√∫sicos, el director, las luces y el escenario, justo antes de la gran noche del estreno.

***

Las pruebas `End-to-End` en una arquitectura de `microservices` buscan validar un flujo de negocio completo o un `user journey` a trav√©s de m√∫ltiples servicios desplegados e interconectados, tal como lo har√≠an en un entorno de producci√≥n. Son el nivel m√°s alto de prueba en la pir√°mide y, aunque costosas, son invaluables.

**¬øQu√© Son las Pruebas `E2E` en un Ecosistema de `Microservices`? üé∂**

* **Definici√≥n:** Pruebas que verifican un `feature` o un flujo de usuario completo desde el punto de entrada del sistema (ej. una llamada al `API Gateway` o una interacci√≥n simulada en la UI) hasta la verificaci√≥n de todos los efectos secundarios esperados a trav√©s de los diversos `microservices` involucrados.
* **Alcance:** Abarcan m√∫ltiples servicios, sus bases de datos, `message brokers`, y cualquier otra infraestructura con la que interact√∫en.
* **Objetivo:**
  * Asegurar que los `microservices` colaboran correctamente para lograr un resultado de negocio.
  * Validar que los contratos (APIs, formatos de mensajes) entre servicios se respetan y funcionan.
  * Detectar problemas de integraci√≥n, configuraci√≥n de red, o despliegue que no se manifiestan en pruebas de menor nivel.
* **Filosof√≠a:** **Minimizar el `mocking`**. Idealmente, se prueba contra instancias reales (o lo m√°s parecidas a producci√≥n posible) de todos los servicios involucrados.

**La Importancia de la Visi√≥n Hol√≠stica (y Sus Intr√≠ngulis) üåü**

* **Por Qu√© Son Necesarias:**
  * **Confianza M√°xima:** Son las que m√°s se asemejan a c√≥mo los usuarios reales experimentar√°n el sistema.
  * **Detecci√≥n de `Bugs` de Integraci√≥n:** Descubren problemas en las "costuras" entre servicios (ej. un servicio env√≠a un `event` en un formato que otro no espera, fallos de comunicaci√≥n en red, inconsistencias de datos que solo se manifiestan en el flujo completo).
  * **Validaci√≥n de `Business Requirements` Completos:** Aseguran que el sistema como un todo cumple con los objetivos de negocio.
* **Los Desaf√≠os Inherentes (No Son un Paseo por el Parque):**
  1. **Complejidad del Entorno de Pruebas (`Test Environment`):**
     * Replicar un entorno con m√∫ltiples `microservices`, sus bases de datos, `brokers` de mensajer√≠a, `service discovery`, etc., es complejo y costoso.
     * Requiere una buena estrategia de `Infrastructure as Code (IaC)` y automatizaci√≥n.
  2. **Lentitud (`Slowness`):**
     * Al involucrar m√∫ltiples servicios, llamadas de red, y operaciones de BBDD, son significativamente m√°s lentas de ejecutar que los `unit` o `integration tests` locales. Esto impacta el `feedback loop` del desarrollador.
  3. **Fragilidad (`Flakiness`):**
     * Con tantas partes m√≥viles, hay m√°s puntos potenciales de fallo que pueden no estar relacionados directamente con un `bug` en el c√≥digo (ej. un problema de red temporal, un servicio dependiente lento, un `timeout` espor√°dico). Esto puede hacer que los tests fallen de forma intermitente y poco fiable.
  4. **Gesti√≥n de Datos de Prueba (`Test Data Management`):**
     * Asegurar un estado de datos conocido y consistente _a trav√©s de m√∫ltiples bases de datos_ antes de cada `test run` es un gran desaf√≠o.
     * La limpieza (`teardown`) o el reseteo de datos tambi√©n es complejo.
  5. **Depuraci√≥n (`Debugging`):**
     * Cuando un test `E2E` falla, identificar el `microservice` o la interacci√≥n espec√≠fica que caus√≥ el problema puede ser como encontrar una aguja en un pajar. Requiere excelente `logging` y `distributed tracing`.
  6. **Coste de Mantenimiento:**
     * Tanto el entorno de pruebas como los `scripts` de prueba `E2E` requieren un esfuerzo de mantenimiento considerable a medida que el sistema evoluciona.

**Estrategias para Abordar las Pruebas `E2E` con √âxito (y Cordura) üó∫Ô∏è**

Dado los desaf√≠os, se necesita una estrategia inteligente:

1. **El Entorno de Pruebas `E2E`:**
   * **Dedicado y Aislado:** Debe ser un entorno espec√≠fico para `E2E testing`, lo m√°s parecido posible a producci√≥n (`staging` o un entorno `pre-production` a menudo se usan).
   * **Automatizaci√≥n del Despliegue:** Usar herramientas como **Docker Compose** (para entornos locales o `CI` m√°s simples) o **Kubernetes** (para entornos m√°s complejos y representativos de producci√≥n) para desplegar todos los `microservices` y su infraestructura de forma reproducible.
2. **Selecci√≥n de Escenarios Cr√≠ticos (`Critical User Journeys` / `Happy Paths`):**
   * **La Pir√°mide de Pruebas:** No intentes cubrir cada peque√±o detalle con pruebas `E2E`. Recuerda la pir√°mide:
     * **Base Ancha:** Muchas pruebas **Unitarias** (r√°pidas, baratas, aisladas).
     * **Medio:** Un buen n√∫mero de pruebas de **Integraci√≥n** (probando la colaboraci√≥n entre un servicio y sus dependencias directas como su BBDD, o la interacci√≥n entre un par de servicios).
     * **Punta Estrecha:** **Pocas, pero muy significativas, pruebas `E2E`** que cubran los flujos de negocio m√°s cr√≠ticos y de mayor valor.
   * Enf√≥cate en los "caminos felices" (`happy paths`) de tus `user journeys` m√°s importantes y algunos "caminos tristes" (`sad paths`) cruciales.
3. **Gesti√≥n de Datos de Prueba (`Test Data Management - TDM`):**
   * **`Seed Data` / Datos de Partida:** Cargar un conjunto de datos base conocidos en las bases de datos de los `microservices` antes de ejecutar la `suite` de pruebas `E2E`.
   * **Aislamiento de Datos por Test:** Idealmente, cada `test case E2E` deber√≠a operar sobre su propio conjunto de datos para evitar interferencias. Esto se puede lograr:
     * Creando datos √∫nicos para cada test (ej. `user_test_123@example.com`).
     * Estrategias de limpieza (`teardown`) despu√©s de cada test o `suite` (puede ser lento).
     * Restaurando las bases de datos a un `snapshot` conocido.
   * **Evitar Dependencias de Estado Preexistente:** Dise√±ar tests que creen los datos que necesitan y los limpien si es posible.
4. **Ejecuci√≥n de la Prueba (Desde una Perspectiva Externa):**
   * Los tests `E2E` se ejecutan simulando un `client` externo.
   * Se usa un cliente HTTP (como `httpx` o `requests` en un `script` `Pytest`) para interactuar con el punto de entrada del sistema (generalmente un `API Gateway`).
   * **Verificaci√≥n del Resultado:**
     * Comprobar la respuesta final de la API.
     * **Y crucialmente:** Verificar los efectos secundarios esperados en el sistema. Esto puede implicar:
       * Realizar `queries` a las APIs de otros `microservices` para verificar su estado.
       * (Con precauci√≥n y como √∫ltimo recurso) Inspeccionar directamente las bases de datos de los servicios (si se tiene acceso controlado en el entorno de E2E).
       * Verificar que ciertos `events` se hayan publicado en `message queues` o `topics`.
5. **FastAPI en el Entorno `E2E`:**
   * Tus `microservices` FastAPI ser√°n componentes desplegados y en ejecuci√≥n dentro de este entorno de pruebas `E2E`.
   * Los `scripts` de prueba `E2E` har√°n peticiones HTTP reales a los `endpoints` de tus servicios FastAPI (probablemente a trav√©s de un `API Gateway`).
   * **Importante:** El `TestClient` de FastAPI (`from fastapi.testclient import TestClient`) se usa para pruebas de integraci√≥n _dentro_ de un solo servicio FastAPI (pruebas `in-process`). **NO se usa para las llamadas HTTP entre servicios distintos y desplegados independientemente en un test `E2E`**. Para eso, usas un cliente HTTP est√°ndar como `httpx`.
6. **Herramientas del Oficio (Menciones Breves) üõ†Ô∏è**
   * **Orquestador de Pruebas:** `pytest` sigue siendo una excelente opci√≥n para escribir y ejecutar tus `scripts` de prueba `E2E`.
   * **Clientes HTTP:** `httpx` (as√≠ncrono, ideal si tus `scripts` de prueba son `async`) o `requests` (s√≠ncrono).
   * **Gesti√≥n de Entorno:** Docker, Docker Compose, Kubernetes.
   * **`Contract Testing` (Ej. Pact, Spring Cloud Contract):** Una t√©cnica complementaria (no un reemplazo total) que puede reducir la necesidad de algunas pruebas `E2E`. Se enfoca en verificar que los "contratos" (formatos de mensajes, signaturas de API) entre servicios se cumplen, sin necesidad de desplegar todos los servicios.

**Diagrama `Mermaid`: Un Flujo de Prueba `E2E` T√≠pico üåê**

Imaginemos un flujo de creaci√≥n de un pedido que involucra un `API Gateway`, `Orders Service`, e `Inventory Service`.

```mermaid
sequenceDiagram
    participant TestRunner as Pytest Test Orchestrator
    participant E2ETestScript as E2E Test Script (using httpx)
    participant APIGW as API Gateway (Deployed)
    participant OrdersSvc as Orders Service (Deployed FastAPI App)
    participant OrdersDB as Orders Database
    participant InventorySvc as Inventory Service (Deployed FastAPI App)
    participant InventoryDB as Inventory Database
    participant EventBus as Message Broker (Optional for this flow)

    TestRunner->>E2ETestScript: Run `test_create_order_and_check_inventory()`
    
    E2ETestScript->>+APIGW: POST /api/orders (Order Payload)
    APIGW->>+OrdersSvc: Forward Request to Create Order Endpoint
    OrdersSvc->>+OrdersDB: Store New Order (Status: PENDING)
    OrdersDB-->>-OrdersSvc: Order Created (ID: 123)
    OrdersSvc-->>EvBus: (Optional) Publish OrderCreatedEvent
    OrdersSvc-->>-APIGW: HTTP 201 Created (Order ID: 123)
    APIGW-->>-E2ETestScript: HTTP 201 Response (Order ID: 123)

    E2ETestScript->>E2ETestScript: Assert: Response Status Code is 201
    E2ETestScript->>E2ETestScript: Assert: Response contains Order ID

    Note over E2ETestScript: (Simulate some delay or wait for event propagation if async)

    EvBus-->>InventorySvc: (If event-driven) OrderCreatedEvent
    InventorySvc->>InventorySvc: (Or OrdersSvc calls InventorySvc API directly)
    InventorySvc->>+InventoryDB: Reserve Stock for Order 123
    InventoryDB-->>-InventorySvc: Stock Reserved

    E2ETestScript->>+APIGW: GET /api/inventory/product_X (To verify stock change)
    APIGW->>+InventorySvc: Forward Request to Get Product Stock Endpoint
    InventorySvc->>+InventoryDB: Read Stock for Product X
    InventoryDB-->>-InventorySvc: Current Stock Data
    InventorySvc-->>-APIGW: HTTP 200 OK (Stock Data)
    APIGW-->>-E2ETestScript: HTTP 200 Response (Stock Data)

    E2ETestScript->>E2ETestScript: Assert: Stock for Product X has decreased
    E2ETestScript->>TestRunner: Test Passed/Failed
```

**Equilibrio y Estrategia: El Lugar de las Pruebas `E2E` en la Pir√°mide ‚öñÔ∏è**

Las pruebas `E2E` son la c√∫spide de la pir√°mide de pruebas. Son las m√°s costosas de escribir, ejecutar y mantener.

* **No Abusar:** No intentes cubrir todas las variaciones y casos l√≠mite con pruebas `E2E`. Eso es trabajo para tus `unit` e `integration tests`.
* **Enfoque en lo Cr√≠tico:** √ösalas para validar los flujos de negocio m√°s importantes y los `user journeys` clave que demuestran que tu sistema, como un todo, entrega valor.

**Conclusi√≥n: La Prueba de Fuego para la Confianza Total en el Sistema üî•**

Las pruebas `End-to-End` son el nivel m√°s alto de validaci√≥n para tu arquitectura de `microservices`. Simulan c√≥mo los usuarios reales o sistemas externos interactuar√°n con tu ecosistema, ofreciendo una confianza inigualable en que todos los componentes, desde el `API Gateway` hasta las bases de datos m√°s profundas y los `message brokers` intermedios, colaboran armoniosamente.

A pesar de sus inherentes desaf√≠os en complejidad, velocidad y mantenimiento, una `suite` bien dise√±ada de pruebas `E2E` es una inversi√≥n invaluable en la calidad y fiabilidad de tu sistema, actuando como la prueba de fuego final antes de que tus `microservices` enfrenten el mundo real.

***

### 14.7 Validaci√≥n de eventos y colas en tests async

Este es un punto fascinante y muy relevante en arquitecturas modernas basadas en `microservices`: **la validaci√≥n de `events` y `queues` (o `topics`) en `tests` as√≠ncronos**. Si nuestros `microservices` se comunican principalmente a trav√©s de mensajes as√≠ncronos (como vimos en el Tema 9), necesitamos estrategias robustas para probar que esta comunicaci√≥n funciona como se espera.

Imagina que tus `events` son mensajes secretos pasados entre agentes encubiertos (tus `microservices`) usando buzones seguros (`queues/topics`). Tus pruebas necesitan ser como contra-esp√≠as expertos, capaces de verificar que los mensajes correctos son enviados, que llegan a los buzones adecuados, y que los agentes receptores los interpretan y act√∫an sobre ellos correctamente.

***

En una arquitectura `event-driven`, donde los servicios reaccionan a `events` en lugar de llamarse directamente de forma s√≠ncrona, el `testing` adquiere nuevos matices. No solo probamos la l√≥gica interna de un servicio, sino tambi√©n su capacidad para:

1. **Producir `Events` Correctamente:** Cuando ocurre una acci√≥n de negocio, ¬øpublica el servicio el `event` esperado, con el `payload` correcto, al `topic` o `exchange` correcto?
2. **Consumir `Events` Correctamente:** Cuando un `event` llega a una `queue` o `topic` que el servicio est√° escuchando, ¬ølo procesa adecuadamente y realiza las acciones subsecuentes esperadas?

**1. El Desaf√≠o: Testear Flujos de `Events` As√≠ncronos üéØ**

La naturaleza as√≠ncrona de la mensajer√≠a introduce desaf√≠os para el `testing`:

* **`Timing` No Determinista:** No sabes _exactamente_ cu√°ndo un `event` publicado ser√° consumido y procesado. Tus `tests` no pueden simplemente asumir que la acci√≥n ha ocurrido inmediatamente despu√©s de la publicaci√≥n.
* **Interacci√≥n con el `Message Broker`:** Para pruebas de integraci√≥n m√°s realistas, necesitas una forma de que tus `tests` interact√∫en o al menos observen el `message broker` (RabbitMQ, Kafka).
* **Aislamiento:** ¬øC√≥mo pruebas un `producer` sin un `consumer` real, o un `consumer` sin un `producer` real? ¬øY c√≥mo pruebas el flujo completo?

**2. Probando el Lado del Productor (`Event Publisher`) üì§**

**Objetivo:** Verificar que tu servicio, tras una acci√≥n espec√≠fica, publica el `event` correcto.

* **Estrategia 1: `Mocking` el Cliente del `Message Broker` (Prueba de Unidad / Integraci√≥n Local)**
  * **Concepto:** En tu `test` para el `service` o `endpoint` que produce el `event`, reemplazas (`mockeas`) el cliente real del `broker` (ej. el objeto `channel` de `aio-pika` o el `AIOKafkaProducer` de `aiokafka`) con un `AsyncMock`.
  * **Verificaci√≥n:** Despu√©s de que tu servicio ejecuta la acci√≥n, haces `assert` de que el m√©todo `publish` (o `send`) del `mock` fue llamado con los argumentos esperados (`exchange/topic`, `routing_key`, `body` del `message`, `headers`).
  * **Pros:**
    * R√°pido y completamente aislado (no se necesita un `broker` real).
    * Control total sobre el entorno.
  * **Cons:**
    * No prueba la configuraci√≥n real del `broker` (ej. si el `exchange` existe y est√° bien configurado en RabbitMQ).
    * No prueba la serializaci√≥n real del `message` tal como ir√≠a por el `wire`.
    * Solo prueba la _intenci√≥n_ de publicar.
  *   **Ejemplo Conceptual (con `pytest-mock` y `AsyncMock` para un productor `aio-pika`):**

      ```python
      # # tests/services/test_order_service_publishes_event.py
      # import pytest
      # from unittest.mock import AsyncMock, ANY # ANY para argumentos que no necesitas verificar en detalle
      # # from app.services.order_service import OrderService
      # # from app.schemas.order_api import OrderCreatePydantic

      # @pytest.mark.asyncio
      # async def test_create_order_publishes_order_created_event(mocker): # mocker es un fixture de pytest-mock
      #     # Arrange
      #     mock_order_repo = AsyncMock() # Mock del repositorio de √≥rdenes
      #     mock_order_repo.save_order.return_value = {"id": "123", "status": "PENDING"} # Simular guardado
          
      #     # Mockear el channel de aio-pika o el exchange directamente
      #     # Si OrderService toma un 'channel' o un 'exchange_publisher'
      #     mock_event_publisher_channel = AsyncMock() 
      #     # Si el m√©todo es channel.default_exchange.publish:
      #     # mock_default_exchange = AsyncMock()
      #     # mock_event_publisher_channel.default_exchange = mock_default_exchange

      #     # Asumimos que OrderService recibe algo para publicar, como un objeto exchange
      #     mock_exchange = AsyncMock()

      #     # order_service = OrderService(order_repo=mock_order_repo, event_exchange=mock_exchange)
      #     # order_data = OrderCreatePydantic(customer_id="cust1", items=[...])
          
      #     # Act
      #     # await order_service.create_new_order(order_data)
          
      #     # Assert
      #     # mock_exchange.publish.assert_awaited_once()
      #     # call_args = mock_exchange.publish.call_args
      #     # Verificamos el mensaje y la routing key
      #     # message_published = call_args[0][0] # El primer argumento posicional (aio_pika.Message)
      #     # routing_key_published = call_args[1]['routing_key'] # El argumento keyword 'routing_key'
          
      #     # import json
      #     # event_payload = json.loads(message_published.body.decode())
      #     # assert routing_key_published == "orders.created"
      #     # assert event_payload["order_id"] == "123"
      #     # assert event_payload["type"] == "OrderCreated"
      #     pass # Este es un placeholder, la implementaci√≥n real depende de c√≥mo se inyecte el publisher
      ```
* **Estrategia 2: Publicar a un `Broker` de Prueba y Verificar el Mensaje en la `Queue/Topic`**
  * **Concepto:** Tu servicio bajo prueba publica a una instancia real (pero de prueba y aislada) de RabbitMQ o Kafka. Tu `test` luego necesita una forma de "espiar" o consumir el `message` de esa `queue/topic` para verificar su contenido.
  * **Implementaci√≥n:**
    1. Configura un `broker` de prueba (ej. Dockerizado, como vimos para la BBDD en 14.5).
    2. En tu `test`, despu√©s de que el servicio productor act√∫a, usas un **`test utility consumer`**: una peque√±a funci√≥n o clase consumidora dentro de tu `test suite` que se suscribe al `topic/queue` esperado.
    3. Este `utility consumer` recibe el `message` (con un `timeout` para no esperar indefinidamente) y lo pone a disposici√≥n del `test` para las `assertions`.
  * **Pros:**
    * Mucha m√°s confianza: prueba la serializaci√≥n real, la conexi√≥n al `broker`, y la configuraci√≥n b√°sica (ej. que el `exchange/topic` exista y est√© ruteando).
  * **Cons:**
    * M√°s lento que `mocking` puro.
    * Requiere infraestructura de `broker` de prueba.
    * La l√≥gica del `utility consumer` (con `timeouts`, etc.) a√±ade algo de complejidad al `test`.
  * **Diagrama `Mermaid`: Test de Productor con `Broker` Real**

**3. Probando el Lado del Consumidor (`Event Consumer/Handler`) üì•**

**Objetivo:** Verificar que tu `consumer`, cuando un `event` espec√≠fico llega a la `queue/topic` que escucha, lo procesa correctamente y realiza los efectos secundarios esperados.

* **Estrategia 1: Prueba Unitaria de la L√≥gica del `Handler` del Mensaje (Aislamiento Total)**
  * **Concepto:** Extraes la l√≥gica de negocio que se ejecuta cuando se recibe un `message` en una funci√≥n o m√©todo separado. En tu `test`, llamas a esta funci√≥n/m√©todo directamente, pas√°ndole un `payload` de `message` simulado (ej. un `dict` o un objeto Pydantic).
  * **`Mocking`:** Todas las dependencias de esta l√≥gica (ej. llamadas a base de datos, a otros `services`) se `mockean`.
  * **Pros y Cons:** Similar al `mocking` del `publisher` (r√°pido, aislado, pero no prueba la interacci√≥n con el `broker` ni la deserializaci√≥n desde el `wire format`).
  * _(Esto es muy similar a las pruebas unitarias de `services` que vimos en 14.2)._
* **Estrategia 2: Integraci√≥n del Consumidor con un `Broker` de Prueba**
  * **Concepto:**
    1. En tu `test`, publicas un `message` de prueba directamente en la `queue/topic` que tu consumidor (la instancia bajo prueba, o una instancia de prueba de tu servicio consumidor) est√° escuchando.
    2. Permites que el consumidor se active y procese el `message` (esto es as√≠ncrono, as√≠ que necesitar√°s `await` o `polling` con `timeout`).
    3. Verificas los efectos secundarios:
       * ¬øCambi√≥ el estado en su base de datos (de prueba)?
       * ¬øLlam√≥ a otros `services` (que podr√≠an estar `mockeados` para este nivel de `test`)?
       * ¬øPublic√≥ nuevos `events` como resultado? (Podr√≠as usar un `test utility consumer` para capturarlos).
  * **Pros:** Prueba la deserializaci√≥n del `message`, la l√≥gica de consumo, y la interacci√≥n con sus dependencias directas (como su BBDD de prueba).
  * **Cons:** M√°s complejo de configurar que el `unit test` del `handler`. Necesita un `broker` de prueba.
  * **Diagrama `Mermaid`: Test de Consumidor con `Broker` Real**

**4. Pruebas `End-to-End` de Flujos de `Events` (El Ciclo Completo) üîÑ**

* **Concepto:** Combinar las pruebas de productor y consumidor.
  1. El `test` desencadena una acci√≥n en el `Service A` (Productor) a trav√©s de su API (usando `TestClient` o `httpx` si `Service A` es una `app` desplegada).
  2. `Service A` publica un `event` en el `broker` de prueba.
  3. `Service B` (Consumidor, tambi√©n una `app` desplegada o instancia de prueba) consume el `event`.
  4. El `test` verifica los efectos secundarios en `Service B` (o incluso en servicios posteriores si es una cadena).
* **Complejidad:** Son las m√°s complejas y lentas, pero dan la mayor confianza en el flujo completo. √ösalas con moderaci√≥n para los `critical paths`. (Esto se alinea con lo visto en 14.6).

**5. T√©cnicas y Herramientas para Pruebas As√≠ncronas de `Events` üõ†Ô∏è**

* **Esperas con `Timeout` y `Polling`:** Dado que la mensajer√≠a es as√≠ncrona, tus `tests` necesitar√°n esperar a que ocurran las cosas.
  * `asyncio.wait_for()`: Para esperar una `coroutine` espec√≠fica con un `timeout`.
  *   Bucles de `Polling` con `Timeout`: Comprobar repetidamente una condici√≥n (ej. un `record` en la BBDD, una llamada a un `mock`) hasta que se cumpla o se agote el `timeout`.

      ```python
      # async def wait_for_condition(condition_func, timeout=5.0, interval=0.1):
      #     start_time = time.monotonic()
      #     while time.monotonic() - start_time < timeout:
      #         if await condition_func():
      #             return True
      #         await asyncio.sleep(interval)
      #     return False

      # # En el test:
      # # assert await wait_for_condition(lambda: mock_service.call_count > 0)
      ```
* **`Test Utility Consumers/Listeners`:** Como se mencion√≥, son peque√±os `consumers` dentro de tu `test suite` que te ayudan a "capturar" `events` para `assertion`. Las librer√≠as cliente (aio-pika, aiokafka) se usan para construirlos.
* **Bibliotecas de Ayuda para `Testing` de `Microservices`:**
  * **`Testcontainers`:** Permite gestionar el ciclo de vida de contenedores Docker (para `brokers`, BBDD) directamente desde tus `tests` Python.
  * `pytest-rabbitmq`, `pytest-kafka`: `Plugins` de `pytest` que pueden ayudar a gestionar `fixtures` para estos `brokers`.

**6. Consideraciones Cruciales: Idempotencia y Reintentos en `Tests`**

* **Probando Idempotencia del Consumidor (ver 9.6):** Dise√±a `tests` que env√≠en el mismo `event` dos o m√°s veces a tu consumidor y verifica que el estado final del sistema sea el correcto (como si se hubiera procesado solo una vez).
* **Probando L√≥gica de Reintentos y `DLQ/DLT` del Consumidor (ver 9.8):**
  * Usa `mocks` para simular fallos transitorios en las dependencias de tu consumidor (ej. `mock_db.update.side_effect = [DBError(), DBError(), success_value]`).
  * Verifica que el consumidor reintenta el n√∫mero esperado de veces.
  * Simula un fallo permanente y verifica que el `message` termina en la `Dead Letter Queue/Topic` configurada (puedes usar un `test utility consumer` para escuchar en la `DLQ/DLT`).

**Conclusi√≥n: Iluminando los Caminos Ocultos de la Mensajer√≠a As√≠ncrona üí°**

Testear arquitecturas `event-driven` requiere un cambio de mentalidad respecto al `testing` de c√≥digo puramente s√≠ncrono y monol√≠tico. La asincron√≠a, la necesidad de interactuar con un `broker` (real o simulado), y la validaci√≥n de efectos secundarios distribuidos son los principales desaf√≠os.

Sin embargo, al combinar estrategias de `mocking` inteligente, el uso de `brokers` de prueba, la creaci√≥n de `test utility consumers`, y una gesti√≥n cuidadosa de `timeouts` y esperas en tus `scripts` de prueba as√≠ncronos, puedes alcanzar un alto grado de confianza en la robustez y correcci√≥n de tus flujos de `events`. Estas pruebas son vitales para asegurar que tus `microservices` desacoplados realmente colaboran como se espera.

***

### 14.8 Cobertura y calidad con `coverage.py`

Despu√©s de aprender a escribir diferentes tipos de `tests` (unitarios, de integraci√≥n), una pregunta natural surge: ¬øQu√© tan bien est√°n cubriendo nuestro c√≥digo estos `tests`? ¬øHay partes de nuestra l√≥gica de negocio que se escapan de nuestra red de seguridad de `testing`?

Aqu√≠ es donde entra en juego la **medici√≥n de la cobertura de pruebas (`test coverage`)**, y la herramienta por excelencia en Python para esto es **`coverage.py`**, a menudo usada en conjunto con `pytest` a trav√©s del `plugin` `pytest-cov`.

***

Escribir `tests` es fundamental. Saber qu√© partes de tu c√≥digo est√°n siendo ejercitadas por esos `tests` es el siguiente nivel para construir software de alta calidad. La cobertura de pruebas te da una visi√≥n cuantitativa de esto, actuando como un mapa que ilumina las √°reas de tu c√≥digo que tus `tests` visitan y, m√°s importante a√∫n, las que dejan en la oscuridad.

**1. `Code Coverage`: El Detective que Revela las Zonas No Exploradas de Tu Aplicaci√≥n üïµÔ∏è‚Äç‚ôÄÔ∏è**

* **¬øQu√© es `Code Coverage`?**\
  Es una m√©trica que indica qu√© porcentaje de tu c√≥digo fuente (l√≠neas, `statements`, `branches`, etc.) es ejecutado cuando tu `test suite` completa se ejecuta. No mide la calidad de las `assertions` ni si los `tests` son significativos, pero s√≠ te dice qu√© c√≥digo _no_ se est√° ejecutando en absoluto durante tus `tests`.
* **¬øPor Qu√© es Importante?**
  * **Identifica C√≥digo No Probado:** Su valor principal es se√±alar las partes de tu aplicaci√≥n que no tienen ninguna cobertura de `test`. Estas son √°reas de alto riesgo para `bugs` ocultos.
  * **Gu√≠a para Escribir M√°s `Tests`:** Te ayuda a enfocar tus esfuerzos de `testing` en las √°reas menos cubiertas o m√°s cr√≠ticas.
  * **Indicador (con Cautela) de la Minuciosidad:** Un `coverage` alto _puede_ ser un indicador de una `test suite` m√°s completa, pero no es una garant√≠a.
  * **Confianza en `Refactoring`:** Saber que tienes un buen `coverage` te da m√°s confianza al refactorizar c√≥digo, ya que los `tests` deber√≠an detectar regresiones.
* **La Trampa de la Cobertura del 100% (¬°No es el Santo Grial!):**
  * Es crucial entender que **100% de `code coverage` NO significa que tu c√≥digo est√© libre de `bugs` o que tus `tests` sean perfectos.** Puedes tener 100% de `line coverage` y a√∫n as√≠ tener `bugs` si tus `assertions` son d√©biles o no cubren todos los `edge cases` l√≥gicos.
  * Perseguir ciegamente el 100% puede llevar a escribir `tests` de bajo valor solo para "pintar de verde" el `report`.
  * **Sin embargo, un `coverage` bajo (ej. < 70-80%) es casi siempre una se√±al de alarma** de que hay porciones significativas de tu c√≥digo sin probar.

**2. `coverage.py`: La Herramienta Est√°ndar de Python para Medir la Cobertura üìè**

* **`coverage.py`** es la herramienta de facto para medir la cobertura de c√≥digo en Python. Funciona instrumentando tu c√≥digo (sin modificar tus fuentes) para rastrear qu√© l√≠neas se ejecutan durante una `test run`.
* **`pytest-cov`:** Para una integraci√≥n perfecta con `pytest`, se usa el `plugin` `pytest-cov`. Este `plugin` maneja la ejecuci√≥n de `coverage.py` alrededor de tu `test suite` `pytest`.

**3. Manos a la Obra: Usando `pytest-cov` con Tu Proyecto FastAPI üõ†Ô∏è**

1.  **Instalaci√≥n:**

    ```bash
    pip install coverage pytest-cov
    ```
2. **Ejecuci√≥n B√°sica para Medir Cobertura:**\
   Cuando ejecutas `pytest`, a√±ades el `flag` `--cov` para especificar qu√© `package` o `module` quieres medir.
   *   Medir cobertura para tu `package` principal (ej. llamado `app`):

       ```bash
       pytest --cov=app
       ```
   *   Para ver un resumen en la terminal con las l√≠neas faltantes:

       ```bash
       pytest --cov=app --cov-report=term-missing
       ```

       Esto mostrar√° una tabla con el porcentaje de cobertura por archivo y las l√≠neas exactas que no fueron cubiertas.
3.  **Generando Informes Detallados:**`pytest-cov` (usando `coverage.py`) puede generar varios tipos de informes:

    *   **Informe HTML (El Mapa Interactivo y M√°s √ötil):**\
        Este es el informe m√°s valioso para el an√°lisis visual.

        ```bash
        pytest --cov=app --cov-report=html
        ```

        Esto crear√° un directorio `htmlcov/` (por defecto). Abre `htmlcov/index.html` en tu `browser`. Ver√°s:

        * Un resumen de cobertura por archivo.
        * Al hacer clic en un archivo, ver√°s tu c√≥digo fuente coloreado:
          * L√≠neas verdes: Ejecutadas por los `tests`.
          * L√≠neas rojas: No ejecutadas.
          * L√≠neas amarillas/naranjas: `Branches` no tomados (si `branch coverage` est√° habilitada).\
            Es una herramienta fant√°stica para identificar exactamente qu√© necesita m√°s `testing`.
    *   **Informe XML (Para Integraci√≥n con Herramientas `CI/CD`):**\
        Usado por herramientas como SonarQube, Codecov, Coveralls.

        ```bash
        pytest --cov=app --cov-report=xml:cov.xml 
        # Esto crea un archivo cov.xml (o el nombre que especifiques)
        ```
    *   **Informe en Terminal (Directo de `coverage.py`):**\
        Despu√©s de ejecutar `pytest --cov=app` (que crea un archivo `.coverage`), puedes usar la CLI de `coverage`:

        ```bash
        coverage report -m  # Muestra resumen y l√≠neas faltantes (-m)
        coverage html       # Genera informe HTML (en htmlcov/)
        coverage xml        # Genera informe XML (en coverage.xml)
        ```

    **Tabla de Comandos/Opciones Comunes (`pytest-cov` / `coverage`):**

    | Comando/Opci√≥n (`pytest` o `coverage CLI`) | Prop√≥sito                                                                                                              |
    | ------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------- |
    | `pytest --cov=PACKAGE_NAME`                | Ejecuta `tests` y mide cobertura para `PACKAGE_NAME`.                                                                  |
    | `pytest --cov-report=html`                 | (Adem√°s de `--cov`) Genera un informe HTML.                                                                            |
    | `pytest --cov-report=xml`                  | (Adem√°s de `--cov`) Genera un informe XML.                                                                             |
    | `pytest --cov-report=term-missing`         | (Adem√°s de `--cov`) Muestra un resumen en terminal con las l√≠neas de c√≥digo exactas que no fueron cubiertas.           |
    | `pytest --cov-branch`                      | (Adem√°s de `--cov`) Habilita la medici√≥n de `branch coverage` (ver m√°s abajo).                                         |
    | `coverage report`                          | Muestra un resumen de cobertura en la terminal (requiere que `coverage run` o `pytest --cov` se haya ejecutado antes). |
    | `coverage report -m`                       | Similar a `report`, pero tambi√©n muestra las l√≠neas faltantes.                                                         |
    | `coverage html [-d DIRNAME]`               | Genera un informe HTML (en el `DIRNAME` especificado o `htmlcov/`).                                                    |
    | `coverage xml [-o FILENAME]`               | Genera un informe XML (al `FILENAME` especificado o `coverage.xml`).                                                   |
    | `coverage erase`                           | Borra los datos de cobertura acumulados (el archivo `.coverage`).                                                      |

**4. Configuraci√≥n Fina con `.coveragerc`: Adaptando la Medici√≥n a Tus Necesidades ‚öôÔ∏è**

Para evitar escribir las mismas opciones de `coverage` cada vez y para configuraciones m√°s complejas, puedes usar un archivo `.coveragerc` en la ra√≠z de tu proyecto.

*   **Ejemplo de un Archivo `.coveragerc`:**

    ```ini
    [run]
    source = app  ; Directorio(s) o package(s) a medir, separados por coma
    branch = True ; Habilitar branch coverage por defecto
    omit = 
        */tests/* ; Excluir todos los archivos dentro de cualquier directorio 'tests'
        app/main.py        ; Ejemplo: Excluir el archivo de arranque si es muy simple
        app/core/config.py ; Ejemplo: Excluir archivos de configuraci√≥n puros
        */migrations/* ; Excluir migraciones de base de datos
        venv/* ; Excluir el entorno virtual
        */__init__.py      ; A menudo los __init__.py no tienen l√≥gica que probar

    [report]
    show_missing = True
    skip_covered = False  ; Mostrar todos los archivos en el resumen, incluso si 100% cubiertos
    fail_under = 80       ; 'coverage report' fallar√° (exit code > 0) si la cobertura total es < 80%
    precision = 2         ; N√∫mero de decimales para los porcentajes

    [html]
    directory = html_coverage_report ; Directorio para el informe HTML

    [xml]
    output = reports/coverage.xml    ; Archivo de salida para el informe XML
    ```

    _Con un `.coveragerc`, simplemente correr `pytest --cov` (sin especificar el `package` si ya est√° en `source`) o `coverage report` aplicar√° estas configuraciones._

**5. Interpretando los Resultados y Mejorando la Calidad (No Solo la Cantidad) üí°**

* **M√°s All√° del Porcentaje Global:** Un 85% de cobertura no te dice _qu√©_ 15% no est√° cubierto. El informe HTML es tu mejor herramienta para ver las l√≠neas espec√≠ficas.
* **`Branch Coverage` (Cobertura de Ramificaciones):**
  * **¬øQu√© es?** Mide si cada posible rama de una estructura de control (ej. `if/else`, `try/except`, bucles con `break` o `continue`) ha sido tomada durante los `tests`. Una l√≠nea con un `if` puede estar cubierta (la l√≠nea del `if` se ejecut√≥), pero si el `test` solo cubre el caso `True` y nunca el `False`, la `branch coverage` lo revelar√°.
  * **¬øPor qu√© es m√°s significativo?** Una alta `line coverage` con baja `branch coverage` indica que tus `tests` no est√°n explorando todos los caminos l√≥gicos de tu c√≥digo.
  * `pytest-cov` a menudo habilita la `branch coverage` por defecto si `coverage.py` est√° configurado para ello (o puedes usar `pytest --cov-branch`).
* **Estrategia de Mejora:**
  1. Ejecuta tus `tests` con `coverage` y genera el informe HTML.
  2. Revisa los archivos con menor cobertura, especialmente aquellos con l√≥gica de negocio cr√≠tica.
  3. Analiza las l√≠neas rojas (no cubiertas) y las ramificaciones amarillas (no tomadas).
  4. Escribe **`tests` significativos** que ejerciten esas partes del c√≥digo y, crucialmente, que **hagan `assertions` sobre el comportamiento esperado** en esos caminos.
  5. No escribas `tests` solo para "aumentar el n√∫mero". Cada `test` debe tener un prop√≥sito claro de validaci√≥n.

**6. Cobertura en `Pipelines` de Integraci√≥n Continua (`CI/CD`) (Un Vistazo a 14.10) üöÄ**

La medici√≥n de cobertura es una pr√°ctica est√°ndar en `CI/CD`:

* **Generar Informes:** El `pipeline` de CI ejecuta los `tests` con `coverage` y genera informes (HTML para artefactos, XML para otras herramientas).
* **`Fail Build` por Baja Cobertura:** Usar la opci√≥n `fail_under` en `.coveragerc` (o un `flag` similar en el comando `coverage report`) puede hacer que el `build` falle si la cobertura cae por debajo de un umbral establecido, manteniendo un est√°ndar de calidad.
* **Integraci√≥n con Servicios de Cobertura:** Herramientas como Codecov, Coveralls, o SonarQube pueden consumir los informes XML, mostrar tendencias de cobertura a lo largo del tiempo, integrarse con `Pull Requests` (mostrando si un PR reduce la cobertura), y ofrecer an√°lisis m√°s profundos.

**Diagrama `Mermaid`: El Proceso de Medici√≥n de Cobertura ‚öôÔ∏è**

```mermaid
graph TD
    A[Pytest Test Suite ¬∑ ejemplo: pytest] --> B[Plugin pytest-cov / coverage.py run]
    B --> C[C√≥digo de la Aplicaci√≥n ¬∑ ejemplo: app con --cov=app]
    C --> D[Archivo de Datos de Cobertura ¬∑ ejemplo: .coverage]
    D --> E[Comandos de Reporte ¬∑ coverage report / html / xml]
    E --> F[Reporte en Terminal ¬∑ resumen, porcentaje, l√≠neas faltantes]
    E --> G[Reporte HTML Interactivo ¬∑ directorio htmlcov]
    E --> H[Reporte XML ¬∑ para CI, SonarQube, Codecov]

    style A fill:#D5F5E3,stroke:#2ECC71
    style B fill:#AED6F1,stroke:#3498DB
    style C fill:#E1BEE7,stroke:#8E44AD
    style D fill:#FFF9C4,stroke:#F1C40F
    style E fill:#FFCDD2,stroke:#C62828
    style F fill:#FADBD8,stroke:#C0392B
    style G fill:#FADBD8,stroke:#C0392B
    style H fill:#FADBD8,stroke:#C0392B

```

**Conclusi√≥n: `Coverage.py` como Tu Linterna en la Oscuridad del C√≥digo No Probado üî¶**

La medici√≥n de la cobertura de pruebas con `coverage.py` y `pytest-cov` es una herramienta indispensable en el arsenal de un desarrollador profesional. No es la meta final de tus `efforts` de `testing`, pero s√≠ una gu√≠a poderosa que te ayuda a:

* **Entender la efectividad** de tu `test suite` para ejercitar tu c√≥digo.
* **Identificar riesgos** en √°reas no probadas.
* **Dirigir tus esfuerzos** para escribir `tests` m√°s significativos.

√ösala sabiamente como un medio para mejorar la calidad y la fiabilidad de tu `software`, no simplemente como un n√∫mero a perseguir. Una base de c√≥digo bien probada, con una cobertura razonable y `tests` de calidad, es una base de c√≥digo en la que puedes confiar para evolucionar y escalar.

***

### 14.9 Estructura de carpetas y fixtures

Ya estamos equipados con `Pytest`, el `TestClient` y sabemos c√≥mo `mockear` y probar diferentes capas. Pero a medida que nuestra `test suite` crece, el orden y la reutilizaci√≥n se vuelven cruciales. ¬øC√≥mo organizamos nuestros archivos de prueba? ¬øC√≥mo evitamos repetir c√≥digo de `setup` y `teardown` una y otra vez?

Aqu√≠ es donde una buena **estructura de carpetas** y el poderoso sistema de **`fixtures` de `Pytest`** entran en juego, transformando nuestro taller de pruebas de un posible caos a un espacio de trabajo eficiente y bien organizado.

***

Una `test suite` bien organizada no solo es m√°s f√°cil de navegar, sino tambi√©n m√°s f√°cil de mantener y extender. `Pytest`, con su sistema de descubrimiento y sus `fixtures`, nos ayuda enormemente en esta tarea.

**1. El Mapa de Tu Territorio de Pruebas: Estructura de Directorios Recomendada üó∫Ô∏è**

Una estructura de directorios clara y consistente es el primer paso.

*   **El Directorio `tests/` en la Ra√≠z del Proyecto:**\
    Esta es la convenci√≥n m√°s extendida. `Pytest` buscar√° autom√°ticamente archivos de prueba (`test_*.py` o `*_test.py`) dentro de este directorio y sus subdirectorios.

    ```
    mi_proyecto_fastapi/
    ‚îú‚îÄ‚îÄ app/                # Tu c√≥digo fuente de la aplicaci√≥n
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îú‚îÄ‚îÄ main.py
    ‚îÇ   ‚îú‚îÄ‚îÄ api/
    ‚îÇ   ‚îú‚îÄ‚îÄ services/
    ‚îÇ   ‚îú‚îÄ‚îÄ models/
    ‚îÇ   ‚îî‚îÄ‚îÄ core/
    ‚îú‚îÄ‚îÄ tests/              # Directorio ra√≠z para todas las pruebas
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py     # Hace que 'tests' sea un package
    ‚îÇ   ‚îú‚îÄ‚îÄ conftest.py     # Fixtures globales/compartidas para todo el proyecto
    ‚îÇ   ‚îú‚îÄ‚îÄ unit/           # Pruebas unitarias
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conftest.py # Fixtures espec√≠ficas para pruebas unitarias (opcional)
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_order_service.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models/
    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ test_user_model.py
    ‚îÇ   ‚îú‚îÄ‚îÄ integration/    # Pruebas de integraci√≥n
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conftest.py # Fixtures espec√≠ficas para pruebas de integraci√≥n (opcional)
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_product_endpoints.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ db/
    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ test_product_repository.py
    ‚îÇ   ‚îî‚îÄ‚îÄ e2e/            # Pruebas End-to-End (si las tienes)
    ‚îÇ       ‚îî‚îÄ‚îÄ test_full_order_flow.py
    ‚îî‚îÄ‚îÄ pyproject.toml
    ```
* **Reflejando (Opcionalmente) la Estructura de Tu Aplicaci√≥n:**\
  Dentro de `tests/unit/` o `tests/integration/`, puedes crear subdirectorios que reflejen la estructura de tu `package` `app/`. Esto ayuda a localizar r√°pidamente las pruebas para un m√≥dulo espec√≠fico.
* **Separaci√≥n por Tipo de Prueba:**\
  Crear subdirectorios como `unit/`, `integration/`, y `e2e/` es una pr√°ctica muy recomendada. Te permite:
  * Ejecutar tipos espec√≠ficos de pruebas (ej. `pytest tests/unit`).
  * Aplicar configuraciones diferentes (ej. `fixtures` de base de datos solo para `integration tests`).
  * Entender r√°pidamente el `scope` de un archivo de prueba.
* **El Archivo M√°gico: `conftest.py` ‚Äì El Hogar de las `Fixtures` Compartidas ‚ú®**
  * `Pytest` trata los archivos llamados `conftest.py` de manera especial.
  * Las `fixtures` y `hooks` definidos en un `conftest.py` est√°n **autom√°ticamente disponibles** para todos los `test files` en el mismo directorio y en todos sus subdirectorios, sin necesidad de importarlos.
  * Un `conftest.py` en la ra√≠z del directorio `tests/` es el lugar ideal para `fixtures` que son globales para todo tu proyecto de pruebas (ej. una `fixture` para el `TestClient` de FastAPI, o para el `engine` de la base de datos de prueba).
  * Puedes tener `conftest.py` en subdirectorios para `fixtures` m√°s locales.

**Diagrama `Mermaid` de una Estructura de Directorios de Pruebas T√≠pica:**

```mermaid
graph TD
    ProjectRoot["Tu Proyecto FastAPI/"] --> AppDir["app/ (C√≥digo Fuente)"]
    ProjectRoot --> TestsDir["tests/ (Todas las Pruebas)"]
    
    TestsDir --> ConftestRoot["conftest.py ‚ú®<br/>(Fixtures Globales: ej. TestClient, DB Engine de Test)"]
    TestsDir --> UnitTests["unit/"]
    TestsDir --> IntegrationTests["integration/"]
    TestsDir --> E2ETests["e2e/ (Opcional)"]

    UnitTests --> UnitServices["services/"]
    UnitServices --> TestOrderService["test_order_service.py<br/>(Usa fixtures de ConftestRoot y/o UnitConftest)"]
    UnitTests --> UnitConftest["(Opcional) conftest.py<br/>(Fixtures solo para /unit)"]
    
    IntegrationTests --> IntAPI["api/"]
    IntAPI --> TestProductEPs["test_product_endpoints.py<br/>(Usa fixtures de ConftestRoot y/o IntConftest)"]
    IntegrationTests --> IntConftest["(Opcional) conftest.py<br/>(Fixtures solo para /integration, ej. DB session de test)"]

    style TestsDir fill:#D5F5E3,stroke:#2ECC71,stroke-width:2px
    style ConftestRoot fill:#FFF9C4,stroke:#F1C40F,font-style:italic
```

**2. `Pytest Fixtures`: Tus Herramientas Pre-Ensambladas para Pruebas Eficientes y Elegantes üõ†Ô∏è**

Las `fixtures` son una de las caracter√≠sticas m√°s potentes de `pytest`. Son funciones que configuran un estado o proveen datos/objetos que tus `tests` necesitan. Piensa en ellas como el sistema de `Dependency Injection` para tus pruebas.

* **¬øQu√© Son Exactamente?**\
  Funciones marcadas con el decorador `@pytest.fixture`. `Pytest` las descubrir√° y las ejecutar√° cuando un `test` las solicite (nombr√°ndolas como un argumento).
*   **Definiendo una `Fixture` Simple:**

    ```python
    # tests/conftest.py
    import pytest

    @pytest.fixture
    def sample_product_data() -> dict:
        """Devuelve un diccionario con datos de un producto de ejemplo."""
        return {"name": "Super Laptop Pro", "price": 1299.99, "category": "electronics"}
    ```
*   **Usando una `Fixture` en un `Test`:**\
    Simplemente incluye el nombre de la `fixture` como un argumento en tu funci√≥n de `test`. `Pytest` se encargar√° de ejecutar la `fixture` y pasar su valor de retorno.

    ```python
    # tests/unit/test_some_logic.py
    # def test_product_processing(sample_product_data): # Pytest inyecta el dict aqu√≠
    #     assert sample_product_data["name"] == "Super Laptop Pro"
    #     # ... l√≥gica de prueba que usa sample_product_data ...
    ```
*   **El Poder del `Scope`: Controlando el Ciclo de Vida y la Reutilizaci√≥n de las `Fixtures` (Tabla Clave):**\
    El par√°metro `scope` en `@pytest.fixture` define con qu√© frecuencia se ejecuta el `setup` (y `teardown`) de la `fixture`.

    | `Scope`        | Se Ejecuta (`Setup`/`Teardown`)...                                             | Ideal Para...                                                                                                              |
    | -------------- | ------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------- |
    | **`function`** | Una vez por cada funci√≥n de `test` que la solicita (este es el **`default`**). | `Setup/teardown` que debe estar completamente aislado por `test`. Ej: `mock objects` frescos, `DB session` con `rollback`. |
    | `class`        | Una vez por cada clase de `test` (para `tests` agrupados en clases).           | `Setup` algo costoso que puede ser compartido por todos los `methods` de una `test class`.                                 |
    | `module`       | Una vez por cada m√≥dulo de `test` (archivo `.py`).                             | `Setup` costoso que puede ser compartido por todos los `tests` en un mismo archivo.                                        |
    | `package`      | Una vez por cada `package` (directorio con `__init__.py`).                     | (Menos com√∫n) `Setup` para un `package` completo de `tests`.                                                               |
    | **`session`**  | Una vez por toda la sesi√≥n de `testing` (todos los `tests` ejecutados).        | `Setup` muy costoso que es seguro compartir globalmente. Ej: `database engine`, instancia del `TestClient` de FastAPI.     |

    ```python
    # tests/conftest.py
    # @pytest.fixture(scope="session")
    # def expensive_resource_setup():
    #     print("\n(SETUP SESSION - Creando recurso caro UNA VEZ)")
    #     resource = {"data": "datos_de_sesion_compartidos"}
    #     yield resource # El recurso se provee a los tests
    #     print("\n(TEARDOWN SESSION - Limpiando recurso caro UNA VEZ)")
    ```
*   **`Yield Fixtures`: `Setup` Antes, `Teardown` Despu√©s (El Ciclo de Vida Completo):**\
    Para `fixtures` que necesitan realizar acciones de limpieza (`teardown`) despu√©s de que los `tests` las hayan usado (ej. cerrar una `database connection`, borrar archivos temporales), se usa `yield` en lugar de `return`.

    * El c√≥digo antes de `yield` es el `setup`.
    * El valor `yielded` es lo que se inyecta en el `test`.
    * El c√≥digo despu√©s de `yield` es el `teardown`, y se ejecuta garantizadamente.

    ```python
    # tests/conftest.py (ejemplo de sesi√≥n de BBDD de 14.5, refinado)
    # import pytest_asyncio
    # from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine, async_sessionmaker
    # from app.db.base_class import Base # Asumiendo que tienes Base para tus modelos SQLAlchemy

    # TEST_SQLALCHEMY_DATABASE_URL = "sqlite+aiosqlite:///:memory:" # Para pruebas r√°pidas

    # @pytest_asyncio.fixture(scope="session") # Engine para toda la sesi√≥n
    # async def test_engine_global():
    #     engine = create_async_engine(TEST_SQLALCHEMY_DATABASE_URL, echo=False)
    #     async with engine.begin() as conn: # Crear tablas una vez por sesi√≥n
    #         await conn.run_sync(Base.metadata.drop_all) # Opcional: asegurar limpieza previa
    #         await conn.run_sync(Base.metadata.create_all)
    #     yield engine
    #     await engine.dispose()

    # @pytest_asyncio.fixture(scope="function") # Sesi√≥n por cada test para aislamiento
    # async def db_test_session(test_engine_global) -> AsyncGenerator[AsyncSession, None]:
    #     # La conexi√≥n se obtiene del engine de sesi√≥n y se maneja por test
    #     async with test_engine_global.connect() as connection:
    #         async with connection.begin() as transaction: # Transacci√≥n por test
    #             # Crear una sesi√≥n vinculada a esta conexi√≥n y transacci√≥n
    #             TestAsyncSessionLocal = async_sessionmaker(bind=connection, class_=AsyncSession, expire_on_commit=False)
    #             async with TestAsyncSessionLocal() as session:
    #                 print("\nSETUP: DB Test Session created, transaction started.")
    #                 yield session # La sesi√≥n se usa en el test
    #                 print("\nTEARDOWN: Rolling back DB Test Session transaction.")
    #                 # El rollback es autom√°tico al salir del 'async with connection.begin()' si no hubo commit
    #                 # o si hubo una excepci√≥n. Si el test hace commit, entonces no habr√° rollback aqu√≠.
    #                 # Para asegurar rollback y aislamiento total por test:
    #                 await transaction.rollback() # Forzar rollback para aislar el test
    #     # La conexi√≥n se cierra al salir del 'async with test_engine_global.connect()'
    ```

    * **Nota:** La estrategia exacta de `transaction` y `rollback` en la `fixture` de `session` puede variar. El objetivo es el aislamiento del `test`.
* **`Autouse Fixtures` (`@pytest.fixture(autouse=True)`):**\
  Si una `fixture` tiene `autouse=True`, se ejecutar√° autom√°ticamente para cada `test` dentro de su `scope`, sin necesidad de que el `test` la liste como argumento. √ösalas con moderaci√≥n, ya que pueden hacer las dependencias de un `test` menos expl√≠citas. √ötiles para `setup/teardown` global (ej. `mockear` `datetime` para todas las pruebas en un m√≥dulo).

**`Fixtures` Comunes para Proyectos FastAPI (Ejemplos en `tests/conftest.py`):**

1.  **`TestClient` para la Aplicaci√≥n FastAPI (`scope="session"` o `scope="module"`):**

    ```python
    # tests/conftest.py
    import pytest
    from fastapi.testclient import TestClient
    from app.main import app # Asume que tu app FastAPI principal est√° en app/main.py

    @pytest.fixture(scope="session")
    def client() -> TestClient:
        # El 'with' statement es √∫til si TestClient implementa __enter__/__exit__ para cleanup,
        # aunque para TestClient b√°sico no es estrictamente necesario para su funcionamiento normal.
        # Devolverlo directamente suele ser suficiente.
        return TestClient(app)
    ```
2. **`Database Session` de Prueba (como el `db_test_session` de arriba, `scope="function"`).**
3. **`Fixtures` para `Mocking` con `pytest-mock`:**\
   El `fixture` `mocker` (del `plugin` `pytest-mock`) ya est√° disponible globalmente si el `plugin` est√° instalado, no necesitas definirlo. Lo pides como argumento: `def test_algo(mocker): ...`.
4.  **`Fixtures` para Datos de Prueba Comunes:**

    ```python
    # tests/conftest.py
    # from app.schemas.user_api import UserCreatePydantic # Tu DTO Pydantic

    # @pytest.fixture
    # def valid_user_create_payload() -> UserCreatePydantic:
    #     return UserCreatePydantic(email="test@example.com", password="securepassword123", full_name="Test User")

    # @pytest.fixture
    # def another_user_payload() -> dict: # Puede devolver un dict tambi√©n
    #     return {"email": "another@example.com", "password": "anotherPassword"}
    ```

    Luego en tu test: `def test_user_creation(client: TestClient, valid_user_create_payload: UserCreatePydantic): ...`

**Conclusi√≥n: `Fixtures` y Organizaci√≥n como Tus Aliados para un `Testing` Escalable y Placentero üß©**

Una buena **estructura de directorios** mantiene tu `test suite` ordenada, predecible y f√°cil de navegar, especialmente a medida que crece. `Pytest` facilita esto con su descubrimiento basado en convenciones y el uso de `conftest.py`.

Las **`Pytest fixtures`** son una herramienta extraordinariamente poderosa y flexible para:

* **Reducir `boilerplate`:** Escribe l√≥gica de `setup/teardown` una vez y reh√∫sala.
* **Gestionar dependencias de prueba:** Provee a tus `tests` con los recursos que necesitan (como `TestClient`, `database sessions`, `mock objects`, datos de prueba) de forma limpia.
* **Mejorar la legibilidad:** Los `tests` se enfocan en la l√≥gica de `Act` y `Assert`, ya que el `Arrange` a menudo se delega a las `fixtures`.
* **Controlar el ciclo de vida (`scope`):** Optimiza la ejecuci√≥n de `setup` costoso.

Invertir tiempo en dise√±ar una buena estructura de pruebas y `fixtures` bien pensadas (especialmente las compartidas en `conftest.py`) es una de las mejores inversiones que puedes hacer para la salud a largo plazo y la eficiencia de tu proceso de `testing`. ¬°Hace que escribir y mantener `tests` sea una tarea mucho m√°s agradable!

***

### 14.10 Automatizaci√≥n en pipelines CI/CD

Llegamos al **14.10**, la culminaci√≥n de nuestro viaje por el `testing`. Ya sabemos c√≥mo escribir `tests` unitarios, de integraci√≥n, `E2E`, y c√≥mo medir su efectividad con `coverage`. Ahora, el paso final es hacer que todo este esfuerzo trabaje para nosotros de forma autom√°tica, continua y fiable: la **automatizaci√≥n en `pipelines` CI/CD (`Continuous Integration / Continuous Delivery or Deployment`)**.

Y como colof√≥n, te proporcionar√© una bibliograf√≠a concisa y espec√≠fica para este Tema 14 de `Testing`.

***

Escribir `tests` es esencial. Ejecutarlos consistentemente y autom√°ticamente con cada cambio en el c√≥digo es lo que transforma el `testing` de una tarea ocasional en una verdadera red de seguridad y un pilar de la calidad del `software`. Esto se logra a trav√©s de los `pipelines` CI/CD.

**1. CI/CD: El Coraz√≥n Palpitante del Desarrollo Moderno (Un Vistazo R√°pido) ‚ù§Ô∏è‚öôÔ∏è**

* **`Continuous Integration (CI)` (Integraci√≥n Continua):**\
  Es la pr√°ctica de integrar los cambios de c√≥digo de m√∫ltiples desarrolladores en un repositorio central de forma frecuente (idealmente, varias veces al d√≠a). Cada integraci√≥n se verifica autom√°ticamente mediante un `build` y la ejecuci√≥n de `tests` automatizados.
  * **Objetivo:** Detectar problemas de integraci√≥n temprano, mejorar la colaboraci√≥n y tener una base de c√≥digo m√°s estable.
* **`Continuous Delivery / Deployment (CD)` (Entrega/Despliegue Continuo):**
  * **`Continuous Delivery`:** Extiende la CI automatizando el `release` del `software` a entornos de prueba (`staging`, `UAT`) despu√©s de que el `build` y los `tests` pasen. El despliegue a producci√≥n puede requerir una aprobaci√≥n manual.
  * **`Continuous Deployment`:** Lleva la automatizaci√≥n un paso m√°s all√°, desplegando autom√°ticamente cada cambio que pasa todas las etapas del `pipeline` (incluyendo `tests`) a producci√≥n.
  * **Objetivo:** Entregar valor a los usuarios de forma m√°s r√°pida, frecuente y fiable.

**2. Las Pruebas como Guardianes del `Pipeline`: Asegurando la Calidad en Cada Paso üõ°Ô∏è**

En un `pipeline` CI/CD, los `tests` automatizados son los **guardianes de la calidad**. Act√∫an como `checkpoints` cr√≠ticos:

* Se ejecutan autom√°ticamente en respuesta a `triggers` (ej. un `git push` a una `branch`, la creaci√≥n de un `pull request`).
* Si los `tests` fallan, el `pipeline` se detiene, previniendo que c√≥digo defectuoso avance a etapas posteriores (como `merge` a la `main branch` o despliegue).
* Proveen `feedback` r√°pido a los desarrolladores.

**3. Integrando `Pytest` y `Coverage.py` en Tu `CI Pipeline` üöÄ**

La mayor√≠a de las herramientas CI/CD permiten ejecutar `scripts` de `shell`. Aqu√≠ es donde ejecutar√°s `pytest` y `coverage`.

* **Comandos Clave en el `Script` de CI (ej. en un archivo `YAML` de GitHub Actions o GitLab CI):**
  1. **`Checkout Code`:** Obtener la √∫ltima versi√≥n del c√≥digo del repositorio.
  2. **Set up Python Environment:** Configurar la versi√≥n de Python.
  3.  **Install Dependencies:**

      ```bash
      pip install -r requirements.txt
      pip install -r requirements-dev.txt # O poetry install --with dev
      ```
  4.  **Run Linters/Formatters (Opcional pero recomendado):**

      ```bash
      flake8 .
      black --check .
      isort --check .
      ```
  5.  **Run Tests with Coverage:**

      ```bash
      pytest \
        --cov=your_app_package_name \ # Reemplaza con el nombre de tu package (ej. 'app')
        --cov-report=xml:coverage.xml \    # Para SonarQube, Codecov, etc.
        --cov-report=html:htmlcov \      # Para revisi√≥n manual (guardar como artifact)
        --cov-report=term-missing \      # Resumen en la consola del CI
        --cov-fail-under=80 \            # Ejemplo: fallar si el coverage es < 80%
        --junitxml=test-results.xml      # Reporte de resultados para el CI
      ```

      * `your_app_package_name`: El directorio principal de tu c√≥digo fuente que quieres medir.
      * `--junitxml`: Muchos sistemas CI pueden parsear este formato para mostrar resultados de `tests`.
      * `--cov-fail-under`: Impone un umbral m√≠nimo de cobertura.
  6. **Upload Artifacts (Opcional):** Guardar los informes HTML de cobertura o los `logs` de `test` para inspecci√≥n.
  7. **Build Docker Image / Deploy (Si los pasos anteriores son exitosos).**
* **Configuraci√≥n del Entorno de CI:**
  * **Variables de Entorno:** Para `database URLs` de prueba, `API keys` de servicios de prueba, `secrets` para `tokens`, etc. Los sistemas CI ofrecen formas seguras de gestionar esto.
  * **Servicios Dependientes:** Si tus `integration tests` o `E2E tests` necesitan una base de datos o un `message broker`, puedes:
    * Usar "services" que provee la plataforma CI (ej. GitHub Actions puede levantar contenedores Redis, PostgreSQL).
    * Usar Docker Compose dentro de tu `CI job` para levantar estos servicios: `docker-compose -f docker-compose.test.yml up -d`.

**4. Etapas T√≠picas de un `CI Pipeline` con `Testing` (Conceptual) üè≠**

Un `pipeline` puede tener varias etapas, ejecut√°ndose secuencialmente o en paralelo:

```mermaid
graph LR
    A[Commit / Pull Request] --> B{Lint & Static Analysis};
    B -- OK --> C{Unit Tests};
    C -- OK --> D{Integration Tests<br>(con DB/Broker de prueba)};
    D -- OK --> E{Build & Security Scans<br>(ej. Docker Image, SAST/DAST)};
    E -- OK --> F((Optional: E2E Tests<br>en Staging));
    F -- OK --> G[Deploy to Staging];
    G -- (Aprobaci√≥n Manual/Auto) --> H[üöÄ Deploy to Production];
    
    B -- Falla --> X[Notificar Fallo / Bloquear Merge];
    C -- Falla --> X;
    D -- Falla --> X;
    E -- Falla --> X;
    F -- Falla --> X;

    style A fill:#D5F5E3,stroke:#2ECC71
    style H fill:#ABEBC6,stroke:#1E8449,font-size:18px
    style X fill:#FFCDD2,stroke:#C62828
```

* **`Lint & Static Analysis`:** `Flake8`, `Black`, `iSort`, `MyPy`. Capturan errores y problemas de estilo temprano.
* **`Unit Tests`:** Deben ser r√°pidos. Son la primera validaci√≥n de la l√≥gica.
* **`Integration Tests`:** M√°s lentos, pueden requerir servicios. Verifican la interacci√≥n entre componentes.
* **`Build Artifacts`:** Crear el `Docker image`, `Python package`, etc.
* **`Security Scans`:** `SAST (Static Application Security Testing)`, `DAST (Dynamic...)`, `dependency vulnerability scans`.
* **`End-to-End Tests`:** (Opcional en el `pipeline` principal por su duraci√≥n, a veces en un `pipeline` separado o nocturno).
* **`Deploy`:** A `staging` para m√°s pruebas, o directamente a producci√≥n.

**5. Herramientas Populares de `CI/CD` (Menci√≥n Breve) üåê**

Hay muchas herramientas excelentes, a menudo con generosos `free tiers` para proyectos `open source` o peque√±os:

* **GitHub Actions:** Integrado directamente en GitHub. Muy popular y potente.
* **GitLab CI/CD:** Integrado en GitLab. Muy completo.
* **Jenkins:** Un veterano `open-source`, altamente configurable y extensible.
* **CircleCI:** Popular por su velocidad y configuraci√≥n basada en `Docker`.
* **Bitbucket Pipelines:** Integrado con Bitbucket.
* **Integraci√≥n con Servicios de An√°lisis de C√≥digo:**
  * **SonarQube:** Para an√°lisis est√°tico avanzado, `code smells`, `security vulnerabilities`, y `tracking` de cobertura.
  * **Codecov / Coveralls:** Para visualizar informes de cobertura, `tracking` hist√≥rico y comentarios en `Pull Requests`.

**Conclusi√≥n del Tema 14: El Ciclo Virtuoso del `Testing` Automatizado y la Entrega Continua üåü**

Automatizar tus `tests` dentro de un `pipeline` CI/CD es donde realmente se materializa el valor de todo tu esfuerzo de `testing`. Transforma el `testing` de una actividad manual y espor√°dica en una **cultura de calidad continua y `feedback` r√°pido**.

* **Beneficios Inmediatos:**
  * Detecci√≥n temprana de `bugs` y regresiones.
  * Mayor confianza al hacer `merge` de c√≥digo o desplegar.
  * `Builds` m√°s estables y predecibles.
  * Ciclos de desarrollo m√°s r√°pidos.
* **Impacto a Largo Plazo:**
  * C√≥digo de mayor calidad y m√°s mantenible.
  * Menos `stress` y "modo bombero" para el equipo.
  * Capacidad para entregar valor a los usuarios de forma m√°s frecuente y segura.

Invertir en un buen `pipeline` CI/CD con `testing` automatizado robusto no es un coste, es una de las mejores inversiones que puedes hacer en la salud y el √©xito a largo plazo de tu proyecto `software`.

***

## Bibliograf√≠a

Aqu√≠ tienes algunas referencias clave y concisas para profundizar en el `testing` con Python y FastAPI:

1. **FastAPI Official Documentation - Testing:**
   * _Enlace:_ [fastapi.tiangolo.com/tutorial/testing/](https://fastapi.tiangolo.com/tutorial/testing/)
   * _Por qu√©:_ La gu√≠a oficial y esencial para probar aplicaciones FastAPI, cubriendo `TestClient` para HTTP y `WebSockets`, `dependency overrides`, y m√°s.
2. **`Pytest` Official Documentation:**
   * _Enlace:_ [docs.pytest.org](https://docs.pytest.org/)
   * _Por qu√©:_ La referencia completa para `pytest`, incluyendo `fixtures`, `markers`, `plugins`, y sus potentes `features`.
3. **`unittest.mock` - Python Standard Library Documentation:**
   * _Enlace:_ [docs.python.org/3/library/unittest.mock.html](https://docs.python.org/3/library/unittest.mock.html)
   * _Por qu√©:_ La documentaci√≥n oficial para la librer√≠a de `mocking` est√°ndar de Python, fundamental para `unit testing`.
4. **`pytest-cov` (Coverage.py Plugin for Pytest) Documentation:**
   * _Enlace:_ [pytest-cov.readthedocs.io](https://pytest-cov.readthedocs.io/)
   * _Por qu√©:_ Para entender c√≥mo integrar `coverage.py` con `pytest` y sus opciones de configuraci√≥n y `reporting`. (Tambi√©n enlaza a la documentaci√≥n de `coverage.py`).
5. **Libro: "Python Testing with pytest, Second Edition"** (por Brian Okken)
   * _Editorial:_ The Pragmatic Programmers
   * _Por qu√©:_ Un libro muy pr√°ctico y bien considerado que profundiza en el uso efectivo de `pytest`, desde lo b√°sico hasta `features` avanzadas y `plugins`.

Esta selecci√≥n te proporciona los recursos fundamentales para dominar el `testing` en el contexto de tus aplicaciones FastAPI. ¬°La pr√°ctica constante y la exploraci√≥n de estos recursos te convertir√°n en un maestro de la calidad del `software`!
